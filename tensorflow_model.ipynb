{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd1e38a-90bd-45ea-9451-be526e8ce768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import express as px\n",
    "import plotly.graph_objs as go\n",
    "# ML libraries - idk which one to use yet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, mean_absolute_error, r2_score\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import cKDTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92a59d-4641-42da-b466-fd1c2c11ad8b",
   "metadata": {},
   "source": [
    "**where i left off (hailey)**\n",
    "- TA said to use supervised neural network instead of linear regression so I chose MLPRegressor but its lowkey ass (see below)\n",
    "- I was thinking predicting relative risk at location/hour/weather would be ideal but idk bc the model isn't looking too hot (this would also require requesting hourly nasa data which i didn't fix get_nasa_power_data to change date to hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedc2883-6e80-41c9-aec4-5f6d35eb4648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NASA API\n",
    "def get_nasa_power_data(lat, lon, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches NASA POWER API data for given latitude, longitude, and time range.\n",
    "\n",
    "    Args:\n",
    "    - lat (float): Latitude of the location.\n",
    "    - lon (float): Longitude of the location.\n",
    "    - start_date (str): Start date in YYYYMMDD format.\n",
    "    - end_date (str): End date in YYYYMMDD format.\n",
    "\n",
    "    Returns:\n",
    "    - Pandas DataFrame with selected weather parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Specify multiple parameters in the API request\n",
    "    parameters = \"PRECSNO,T2MDEW,PRECTOTCORR,T2M,WS2M\"\n",
    "\n",
    "    url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "    params = {\n",
    "        \"parameters\": parameters,\n",
    "        \"community\": \"RE\",\n",
    "        \"longitude\": lon,\n",
    "        \"latitude\": lat,\n",
    "        \"start\": start_date,\n",
    "        \"end\": end_date,\n",
    "        \"format\": \"JSON\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # Convert JSON response to DataFrame and transpose it\n",
    "    nasa_weather = pd.DataFrame.from_dict(data[\"properties\"][\"parameter\"], orient=\"index\").T\n",
    "\n",
    "    # Reset index and rename date column\n",
    "    nasa_weather.reset_index(inplace=True)\n",
    "    nasa_weather.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "\n",
    "    # Convert date column to proper datetime format\n",
    "    nasa_weather[\"date\"] = pd.to_datetime(nasa_weather[\"date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    nasa_weather.dropna(subset=[\"date\"], inplace=True)  # Remove invalid date rows\n",
    "\n",
    "    nasa_weather.rename(columns={\n",
    "        \"PRECSNO\": \"Snow_Precipitation\",\n",
    "        \"T2MDEW\": \"Dew_Point_2m\",\n",
    "        \"PRECTOTCORR\": \"Total_Precipitation_mm\",\n",
    "        \"T2M\": \"Temperature_2m_C\",\n",
    "        \"WS2M\": \"Wind_Speed_2m\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add Rounded_Lat and Rounded_Lng for merging\n",
    "    nasa_weather['Rounded_Lat'] = lat\n",
    "    nasa_weather['Rounded_Lng'] = lon\n",
    "    \n",
    "    # Display DataFrame\n",
    "    print(f\"\\n Weather Data for Latitude {lat}, Longitude {lon}\\n\")\n",
    "    print(f\"\\n Weather Data for Latitude {lat}, Longitude {lon}\\n\")\n",
    "    nasa_weather['Precipitation(in)'] = nasa_weather['Total_Precipitation_mm'] / 25.4 # mm to in\n",
    "    nasa_weather['Temperature(F)'] = (nasa_weather['Temperature_2m_C'] * (9./5.)) + 32. # C to F\n",
    "    nasa_weather['Wind_Speed(mph)'] = nasa_weather['Wind_Speed_2m'] * 2.237 # m/s to mph\n",
    "    # nasa_weather.dropna()\n",
    "    display(nasa_weather)  # Works in Jupyter Notebook\n",
    "\n",
    "    return nasa_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9938d54-cead-4cf2-a65f-b238d91be651",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weather Data for Latitude 34.05, Longitude -118.25\n",
      "\n",
      "\n",
      " Weather Data for Latitude 34.05, Longitude -118.25\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Snow_Precipitation</th>\n",
       "      <th>Dew_Point_2m</th>\n",
       "      <th>Total_Precipitation_mm</th>\n",
       "      <th>Temperature_2m_C</th>\n",
       "      <th>Wind_Speed_2m</th>\n",
       "      <th>Rounded_Lat</th>\n",
       "      <th>Rounded_Lng</th>\n",
       "      <th>Precipitation(in)</th>\n",
       "      <th>Temperature(F)</th>\n",
       "      <th>Wind_Speed(mph)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>12.14</td>\n",
       "      <td>1.64</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>53.852</td>\n",
       "      <td>3.66868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>11.55</td>\n",
       "      <td>1.59</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>52.790</td>\n",
       "      <td>3.55683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>5.87</td>\n",
       "      <td>10.86</td>\n",
       "      <td>3.25</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.231102</td>\n",
       "      <td>51.548</td>\n",
       "      <td>7.27025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9.76</td>\n",
       "      <td>2.89</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>49.568</td>\n",
       "      <td>6.46493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.440</td>\n",
       "      <td>4.47400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10.50</td>\n",
       "      <td>2.86</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>50.900</td>\n",
       "      <td>6.39782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>7.77</td>\n",
       "      <td>5.29</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>45.986</td>\n",
       "      <td>11.83373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Snow_Precipitation  Dew_Point_2m  Total_Precipitation_mm  \\\n",
       "0 2024-01-01                 0.0          6.12                    0.04   \n",
       "1 2024-01-02                 0.0          7.05                    0.09   \n",
       "2 2024-01-03                 0.0          6.75                    5.87   \n",
       "3 2024-01-04                 0.0          2.14                    0.02   \n",
       "4 2024-01-05                 0.0          1.99                    0.00   \n",
       "5 2024-01-06                 0.0          1.23                    0.12   \n",
       "6 2024-01-07                 0.0         -0.09                    0.26   \n",
       "\n",
       "   Temperature_2m_C  Wind_Speed_2m  Rounded_Lat  Rounded_Lng  \\\n",
       "0             12.14           1.64        34.05      -118.25   \n",
       "1             11.55           1.59        34.05      -118.25   \n",
       "2             10.86           3.25        34.05      -118.25   \n",
       "3              9.76           2.89        34.05      -118.25   \n",
       "4             10.80           2.00        34.05      -118.25   \n",
       "5             10.50           2.86        34.05      -118.25   \n",
       "6              7.77           5.29        34.05      -118.25   \n",
       "\n",
       "   Precipitation(in)  Temperature(F)  Wind_Speed(mph)  \n",
       "0           0.001575          53.852          3.66868  \n",
       "1           0.003543          52.790          3.55683  \n",
       "2           0.231102          51.548          7.27025  \n",
       "3           0.000787          49.568          6.46493  \n",
       "4           0.000000          51.440          4.47400  \n",
       "5           0.004724          50.900          6.39782  \n",
       "6           0.010236          45.986         11.83373  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Fetch data for different locations\n",
    "df_la = get_nasa_power_data(34.05, -118.25, \"20240101\", \"20240107\")  # Los Angeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efa465c-3a70-4b5f-bf78-ccc26b0b5fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lng</th>\n",
       "      <th>Distance(mi)</th>\n",
       "      <th>...</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Civil_Twilight</th>\n",
       "      <th>Nautical_Twilight</th>\n",
       "      <th>Astronomical_Twilight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-1</td>\n",
       "      <td>Source2</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 05:46:00</td>\n",
       "      <td>2016-02-08 11:00:00</td>\n",
       "      <td>39.865147</td>\n",
       "      <td>-84.058723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-2</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:07:59</td>\n",
       "      <td>2016-02-08 06:37:59</td>\n",
       "      <td>39.928059</td>\n",
       "      <td>-82.831184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-3</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:49:27</td>\n",
       "      <td>2016-02-08 07:19:27</td>\n",
       "      <td>39.063148</td>\n",
       "      <td>-84.032608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-4</td>\n",
       "      <td>Source2</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 07:23:34</td>\n",
       "      <td>2016-02-08 07:53:34</td>\n",
       "      <td>39.747753</td>\n",
       "      <td>-84.205582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-5</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 07:39:07</td>\n",
       "      <td>2016-02-08 08:09:07</td>\n",
       "      <td>39.627781</td>\n",
       "      <td>-84.188354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Source  Severity           Start_Time             End_Time  \\\n",
       "0  A-1  Source2         3  2016-02-08 05:46:00  2016-02-08 11:00:00   \n",
       "1  A-2  Source2         2  2016-02-08 06:07:59  2016-02-08 06:37:59   \n",
       "2  A-3  Source2         2  2016-02-08 06:49:27  2016-02-08 07:19:27   \n",
       "3  A-4  Source2         3  2016-02-08 07:23:34  2016-02-08 07:53:34   \n",
       "4  A-5  Source2         2  2016-02-08 07:39:07  2016-02-08 08:09:07   \n",
       "\n",
       "   Start_Lat  Start_Lng  End_Lat  End_Lng  Distance(mi)  ... Roundabout  \\\n",
       "0  39.865147 -84.058723      NaN      NaN          0.01  ...      False   \n",
       "1  39.928059 -82.831184      NaN      NaN          0.01  ...      False   \n",
       "2  39.063148 -84.032608      NaN      NaN          0.01  ...      False   \n",
       "3  39.747753 -84.205582      NaN      NaN          0.01  ...      False   \n",
       "4  39.627781 -84.188354      NaN      NaN          0.01  ...      False   \n",
       "\n",
       "  Station   Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset  \\\n",
       "0   False  False           False          False        False          Night   \n",
       "1   False  False           False          False        False          Night   \n",
       "2   False  False           False           True        False          Night   \n",
       "3   False  False           False          False        False          Night   \n",
       "4   False  False           False           True        False            Day   \n",
       "\n",
       "  Civil_Twilight Nautical_Twilight Astronomical_Twilight  \n",
       "0          Night             Night                 Night  \n",
       "1          Night             Night                   Day  \n",
       "2          Night               Day                   Day  \n",
       "3            Day               Day                   Day  \n",
       "4            Day               Day                   Day  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect us_accident data\n",
    "us_accidents = pd.read_csv('US_Accidents_March23.csv')\n",
    "us_accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a74597-2201-4f2c-9b95-cbba4120d711",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Light Rain' 'Overcast' 'Mostly Cloudy' 'Rain' 'Light Snow' 'Haze'\n",
      " 'Scattered Clouds' 'Partly Cloudy' 'Clear' 'Snow'\n",
      " 'Light Freezing Drizzle' 'Light Drizzle' 'Fog' 'Shallow Fog' 'Heavy Rain'\n",
      " 'Light Freezing Rain' 'Cloudy' 'Drizzle' nan 'Light Rain Showers' 'Mist'\n",
      " 'Smoke' 'Patches of Fog' 'Light Freezing Fog' 'Light Haze'\n",
      " 'Light Thunderstorms and Rain' 'Thunderstorms and Rain' 'Fair'\n",
      " 'Volcanic Ash' 'Blowing Sand' 'Blowing Dust / Windy' 'Widespread Dust'\n",
      " 'Fair / Windy' 'Rain Showers' 'Mostly Cloudy / Windy'\n",
      " 'Light Rain / Windy' 'Hail' 'Heavy Drizzle' 'Showers in the Vicinity'\n",
      " 'Thunderstorm' 'Light Rain Shower' 'Light Rain with Thunder'\n",
      " 'Partly Cloudy / Windy' 'Thunder in the Vicinity' 'T-Storm'\n",
      " 'Heavy Thunderstorms and Rain' 'Thunder' 'Heavy T-Storm' 'Funnel Cloud'\n",
      " 'Heavy T-Storm / Windy' 'Blowing Snow' 'Light Thunderstorms and Snow'\n",
      " 'Heavy Snow' 'Low Drifting Snow' 'Light Ice Pellets' 'Ice Pellets'\n",
      " 'Squalls' 'N/A Precipitation' 'Cloudy / Windy' 'Light Fog' 'Sand'\n",
      " 'Snow Grains' 'Snow Showers' 'Heavy Thunderstorms and Snow'\n",
      " 'Rain / Windy' 'Heavy Rain / Windy' 'Heavy Ice Pellets'\n",
      " 'Light Snow / Windy' 'Heavy Freezing Rain' 'Small Hail'\n",
      " 'Heavy Rain Showers' 'Thunder / Windy' 'Drizzle and Fog'\n",
      " 'T-Storm / Windy' 'Blowing Dust' 'Smoke / Windy' 'Haze / Windy' 'Tornado'\n",
      " 'Light Drizzle / Windy' 'Widespread Dust / Windy' 'Wintry Mix'\n",
      " 'Wintry Mix / Windy' 'Light Snow with Thunder' 'Fog / Windy'\n",
      " 'Snow and Thunder' 'Light Snow Shower' 'Sleet' 'Light Snow and Sleet'\n",
      " 'Snow / Windy' 'Rain Shower' 'Snow and Sleet' 'Light Sleet'\n",
      " 'Heavy Snow / Windy' 'Freezing Drizzle' 'Light Freezing Rain / Windy'\n",
      " 'Thunder / Wintry Mix' 'Blowing Snow / Windy' 'Freezing Rain'\n",
      " 'Light Snow and Sleet / Windy' 'Snow and Sleet / Windy' 'Sleet / Windy'\n",
      " 'Heavy Freezing Rain / Windy' 'Squalls / Windy'\n",
      " 'Light Rain Shower / Windy' 'Snow and Thunder / Windy'\n",
      " 'Light Sleet / Windy' 'Sand / Dust Whirlwinds' 'Mist / Windy'\n",
      " 'Drizzle / Windy' 'Duststorm' 'Sand / Dust Whirls Nearby'\n",
      " 'Thunder and Hail' 'Heavy Sleet' 'Freezing Rain / Windy'\n",
      " 'Light Snow Shower / Windy' 'Partial Fog' 'Thunder / Wintry Mix / Windy'\n",
      " 'Patches of Fog / Windy' 'Rain and Sleet' 'Light Snow Grains'\n",
      " 'Partial Fog / Windy' 'Sand / Dust Whirlwinds / Windy'\n",
      " 'Heavy Snow with Thunder' 'Light Snow Showers' 'Heavy Blowing Snow'\n",
      " 'Light Hail' 'Heavy Smoke' 'Heavy Thunderstorms with Small Hail'\n",
      " 'Light Thunderstorm' 'Heavy Freezing Drizzle' 'Light Blowing Snow'\n",
      " 'Thunderstorms and Snow' 'Dust Whirls' 'Rain Shower / Windy'\n",
      " 'Sleet and Thunder' 'Heavy Sleet and Thunder' 'Drifting Snow / Windy'\n",
      " 'Shallow Fog / Windy' 'Thunder and Hail / Windy' 'Heavy Sleet / Windy'\n",
      " 'Sand / Windy' 'Heavy Rain Shower / Windy' 'Blowing Snow Nearby'\n",
      " 'Heavy Rain Shower' 'Drifting Snow']\n"
     ]
    }
   ],
   "source": [
    "unique_values = us_accidents['Weather_Condition'].unique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e57d6c3-abd1-4744-a871-1e401bf3aee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of all columns in US Accident Data\n",
    "drop_cols = ['ID',\n",
    "            'Source',\n",
    "            # 'Severity', # Severity = target column, 1-4, where 1 indicates the least impact on traffic\n",
    "            'Start_Time',\n",
    "            'End_Time',\n",
    "            'Start_Lat',  \n",
    "            'Start_Lng', \n",
    "            'End_Lat',\n",
    "            'End_Lng',\n",
    "            'Distance(mi)', # Distance(mi) = target column?, length of road extent affected by accident in miles\n",
    "            'Description', # Description = human description of accident\n",
    "            'Street', \n",
    "            'City', \n",
    "            'County',\n",
    "            'State',\n",
    "            'Zipcode',\n",
    "            'Country',\n",
    "            'Timezone',\n",
    "            'Airport_Code',\n",
    "            'Weather_Timestamp', # Weather_Timestamp = shows time-stamp of weather observation record (in local time)\n",
    "            # 'Temperature(F)',\n",
    "            'Wind_Chill(F)',\n",
    "            'Humidity(%)',\n",
    "            'Pressure(in)',\n",
    "            'Visibility(mi)',\n",
    "            'Wind_Direction',\n",
    "            # 'Wind_Speed(mph)',\n",
    "            # 'Precipitation(in)',\n",
    "            # 'Weather_Condition',\n",
    "            'Amenity',\n",
    "            'Bump',\n",
    "            'Crossing',\n",
    "            'Give_Way',\n",
    "            'Junction',\n",
    "            'No_Exit',\n",
    "            'Railway',\n",
    "            'Roundabout',\n",
    "            'Station',\n",
    "            'Stop',\n",
    "            'Traffic_Calming',\n",
    "            'Traffic_Signal',\n",
    "            'Turning_Loop',\n",
    "            'Sunrise_Sunset', # day or night based on sunrise/sunset\n",
    "            'Civil_Twilight', # day or night based on civil twilight\n",
    "            'Nautical_Twilight', # day or night based on nautical twilight\n",
    "            'Astronomical_Twilight'] # day or night based on astronomical twilight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d4b6a1-e2da-40a3-939c-10d46e3eca18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# string feature that if kept will need to be encoded for ML\n",
    "str_features = 'Weather_Condition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "392196e6-5ad4-4765-987d-ff54fe162e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, split, predictors=[], target=[]):\n",
    "    \"\"\"\n",
    "    Prepares the US Accidents DataFrame for merging with NASA weather data, keeping only necessary columns.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): Raw US Accidents dataset.\n",
    "    - split (boolean): if true, split df by target and predictor data\n",
    "    - ml_drop (list): other columns to drop for machine learning model (adjustable if we decide a variable is not good at predicting)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame with 'date', 'Rounded_Lat', 'Rounded_Lng', and 'Severity' columns.\n",
    "    \"\"\"\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Convert time columns to datetime format\n",
    "    df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "\n",
    "    # Remove rows with invalid 'Start_Time' values\n",
    "    df = df[df['Start_Time'].notnull()].copy()\n",
    "\n",
    "    # Extract 'date' from 'Start_Time' for merging with NASA weather data\n",
    "    # df['date'] = df['Start_Time'].dt.date\n",
    "\n",
    "    # Filter for coordinates within LA County\n",
    "    df = df[(df['Start_Lat'].between(33.7, 34.8)) & (df['Start_Lng'].between(-119.0, -117.6))]\n",
    "\n",
    "    # Round latitude and longitude to 2 decimal places for approximate matching\n",
    "    df['Rounded_Lat'] = df['Start_Lat'].round(2)\n",
    "    df['Rounded_Lng'] = df['Start_Lng'].round(2)\n",
    "    \n",
    "    # encode str_features\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['Weather_Condition'] = le.fit_transform(df['Weather_Condition'])\n",
    "    \n",
    "    # add new columns for increased risk with associated conditions and time\n",
    "    # for now will start by increased risk with associated weather and time\n",
    "    df['hour'] = df['Start_Time'].astype(str).str.split(' ').str.get(1).str.split(':').str.get(0)\n",
    "    \n",
    "    # INCREASED SEVERITY RISK BASED ON LOCATION AND TIME  --------------------------------------------------------------------------\n",
    "    \n",
    "    # add col to represent frequency of accidents at this location and time\n",
    "    df['lt_frequency'] = df.groupby(['Rounded_Lat','Rounded_Lng','hour'])['Severity'].transform('count')\n",
    "\n",
    "    # add col to represent frequency rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lt_frequency_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','hour'])['lt_frequency'].rank()\n",
    "\n",
    "    # add col to represent severity rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lt_severity_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','hour'])['Severity'].rank()\n",
    "    \n",
    "    # INCREASED SEVERITY RISK BASED ON LOCATION AND WEATHER  --------------------------------------------------------------------------\n",
    "    \n",
    "    # add col to represent frequency of accidents at this location and time\n",
    "    df['lw_frequency'] = df.groupby(['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'])['Severity'].transform('count')\n",
    "\n",
    "    # add col to represent frequency rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lw_frequency_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'])['lw_frequency'].rank()\n",
    "\n",
    "    # add col to represent severity rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lw_severity_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'])['Severity'].rank()\n",
    "    \n",
    "    # Compute Risk Changes-----------------------------------------------------------------------------------\n",
    "    \n",
    "    # Average accident frequency per hour (time baseline)\n",
    "    avg_hourly_accidents = df.groupby('hour')['lt_frequency'].transform('mean')\n",
    "    df['time_risk_change'] = (df['lt_frequency'] - avg_hourly_accidents) / avg_hourly_accidents\n",
    "\n",
    "    # Average accident frequency per weather condition (weather baseline)\n",
    "    avg_weather_accidents = df.groupby('Weather_Condition')['lt_frequency'].transform('mean')\n",
    "    df['weather_risk_change'] = (df['lt_frequency'] - avg_weather_accidents) / avg_weather_accidents\n",
    "\n",
    "    # Combined risk factor (balancing time & weather risks)\n",
    "    df['combined_risk'] = 0.5 * df['time_risk_change'] + 0.5 * df['weather_risk_change']\n",
    "\n",
    "    # drop NaN\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # for machine learning clean and prep\n",
    "    if split:\n",
    "        X=df[predictors]\n",
    "        y=df[target]\n",
    "        return X,y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e76987-71ca-417e-a022-59014571d039",
   "metadata": {},
   "source": [
    "### MLPRegressor\n",
    "\n",
    "- sklearn documentation https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "- lowkey don't know good risk metric\n",
    "- what do we want to predict?\n",
    "- we could have 2 different models that relative accident frequency based off of location and weather and location and time (hour) --> predicts lw_frequency_rank and lt_frequency_rank (i think predicting relative accident frequency is more intuitive than relative accident severity)\n",
    "- we could have 2 different models that relative accident severity based off of location and weather and location and time (hour) --> predicts lw_severity_rank and lt_severity_rank\n",
    "- we could have 4 different models that do both above ^\n",
    "- we could come up with a new metric that combines all and only have one model (i just don't know best way to represent that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67570ed0-ae98-46eb-83e0-1fc7fe66b022",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(us_accidents, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63246330-537e-4ec8-8042-3051bc998831",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model to predict relative accident frequency based off of location and time\n",
    "X, y = prepare_data(df=train, split=True, predictors=['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'], target='lw_frequency_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0a9073-c7fb-4564-9529-870686750d85",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24199856265614483"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also takes forever\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "lt_regr = MLPRegressor(random_state=1, solver='lbfgs',max_iter=3000, tol=0.02, hidden_layer_sizes=200, alpha=0.005, )\n",
    "lt_regr.fit(X_train, y_train)\n",
    "lt_regr.predict(X_test)\n",
    "lt_regr.score(X_test, y_test) # sad scores :(\n",
    "# increasing max_iter seems to increase the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca0394-21e7-4bf2-b99b-15439339542c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "idk... i tried to make it faster by only picking 10 sets of parameters instead of testing everything and not using grid search because that may be making it slower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f2367f-fb6c-4454-8b4d-a643f23b66a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'tol': 0.2, 'max_iter': 1000, 'hidden_layer_sizes': (300,), 'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Scale features to normilize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define model, settings optimize speed\n",
    "mlp = MLPRegressor(solver='adam', \n",
    "                   activation='relu', \n",
    "                   batch_size=128, \n",
    "                   early_stopping=True, # prevents unnecessary training\n",
    "                   random_state=1)\n",
    "\n",
    "# Define parameter space\n",
    "param_dist = {\n",
    "    \"max_iter\": [1000, 2000, 3000],\n",
    "    \"hidden_layer_sizes\": [(100,), (200,), (300,)],\n",
    "    \"alpha\": [0.0001, 0.001, 0.005],\n",
    "    \"tol\": [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "688a7442-6d82-4fc0-8b0a-ad26ecdc0a0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ideally once best parameters are found and target prediction is confirmed we just predict whatever using nasa power data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce16ecd-2804-4855-9770-33d12ab3ffdf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Accident frequency depends more on time and location (e.g., rush hour, high-traffic areas), while severity is influenced by weather conditions and road characteristics (e.g., rain, fog, wind speed). By using two separate models, one for frequency and one for severity, we can train each model with the most relevant features, leading to more accurate predictions.\n",
    "\n",
    "But this also takes forever to run so im not sure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8ca0fd-6876-4c52-8a3c-7e8fd4160bb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Model MAE: 13.901943663841028\n",
      "Frequency Model score: 0.3365396492233501\n",
      "Severity Model - MAE: 5.283131020659158\n",
      "Severity Model - R²: 0.3410470146070598\n"
     ]
    }
   ],
   "source": [
    "# MODEL 1: Predicting Accident Frequency\n",
    "# learns from past accident data to preduct how frequently accidents occur at a given\n",
    "# time and location\n",
    "# select input features and target\n",
    "predictors_freq = ['Rounded_Lat', 'Rounded_Lng', 'hour']\n",
    "target_freq = 'lt_frequency'\n",
    "\n",
    "# Prepare the data\n",
    "X_freq, y_freq = prepare_data(train, split=True, predictors=predictors_freq, target=target_freq)\n",
    "\n",
    "#Split into training and test sets\n",
    "X_train_freq, X_test_freq, y_train_freq, y_test_freq = train_test_split(X_freq, y_freq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_freq = MinMaxScaler()\n",
    "X_train_freq_scaled = scaler_freq.fit_transform(X_train_freq)\n",
    "X_test_freq_scaled = scaler_freq.transform(X_test_freq)\n",
    "\n",
    "# Train Neural Network to predict accident frequency\n",
    "freq_model = MLPRegressor(hidden_layer_sizes=(64, 32), \n",
    "                          max_iter=2000, solver='adam', \n",
    "                          activation='relu', \n",
    "                          batch_size=256, \n",
    "                          early_stopping=True, \n",
    "                          random_state=1)\n",
    "\n",
    "# Train the model\n",
    "freq_model.fit(X_train_freq_scaled, y_train_freq)\n",
    "\n",
    "# Test on unseen Data\n",
    "y_pred_freq = freq_model.predict(X_test_freq_scaled)\n",
    "\n",
    "# Print Accuracy Results\n",
    "print(\"Frequency Model MAE:\", mean_absolute_error(y_test_freq, y_pred_freq))\n",
    "print(\"Frequency Model score:\", r2_score(y_test_freq, y_pred_freq))\n",
    "\n",
    "# MODEL 2: Predicting Accident Severity\n",
    "# select features and target for severity model\n",
    "predictors_sev = ['Rounded_Lat', 'Rounded_Lng', 'hour']\n",
    "target_sev = 'lt_severity_rank'\n",
    "\n",
    "# Prepare data\n",
    "X_sev, y_sev = prepare_data(train, split=True, predictors=predictors_sev, target=target_sev)\n",
    "\n",
    "# Split into training and testing set\n",
    "X_train_sev, X_test_sev, y_train_sev, y_test_sev = train_test_split(X_sev, y_sev, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_sev = MinMaxScaler()\n",
    "X_train_sev_scaled = scaler_sev.fit_transform(X_train_sev)\n",
    "X_test_sev_scaled = scaler_sev.transform(X_test_sev)\n",
    "\n",
    "# Train Neural Network for Severity Prediction\n",
    "sev_model = MLPRegressor(hidden_layer_sizes=(64, 32), \n",
    "                         max_iter=2000, \n",
    "                         solver='adam', \n",
    "                         activation='relu', \n",
    "                         batch_size=256, \n",
    "                         early_stopping=True, \n",
    "                         random_state=1)\n",
    "\n",
    "# Train model\n",
    "sev_model.fit(X_train_sev_scaled, y_train_sev)\n",
    "\n",
    "# Test model\n",
    "y_pred_sev = sev_model.predict(X_test_sev_scaled)\n",
    "\n",
    "# Print Accuracy\n",
    "print(\"Severity Model - MAE:\", mean_absolute_error(y_test_sev, y_pred_sev))\n",
    "print(\"Severity Model - R²:\", r2_score(y_test_sev, y_pred_sev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623784d-9d51-4737-ba2e-bd35c6f0d4ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "I got your model to run too...finally haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf613de-c215-4a4e-b9a4-76af1a73168a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this takes forever maybe don't run it\n",
    "regr = MLPRegressor(random_state=1, solver='lbfgs')\n",
    "parameters = {\"max_iter\": list(range(2000,5000,1000)), \n",
    "              \"hidden_layer_sizes\": list(range(100, 500, 100)),\n",
    "             \"tol\": [0.1,0.2,0.3,0.4,0.5],\n",
    "             \"alpha\": [0.0001, 0.0005, 0.001, 0.005, 0.01],}\n",
    "cv_scores = [] #holds the cross validation scores\n",
    "#try every combination of estimators and max depth, recording them and their scores\n",
    "for mi in parameters['max_iter']:\n",
    "    for hls in parameters['hidden_layer_sizes']:\n",
    "        for t in parameters['tol']:\n",
    "            for a in parameters['alpha']:\n",
    "                regr.set_params(solver='lbfgs', max_iter=mi, hidden_layer_sizes=hls, tol=t, alpha=a)\n",
    "                scores = cross_val_score(regr, X_train, y_train, cv=5) #gets cross validation score\n",
    "                \n",
    "                cv_scores.append({\"max_iter\":mi,\n",
    "                                 \"hidden_layer_sizes\":hls,\n",
    "                                 \"tol\":t,\n",
    "                                 \"alpha\":a})\n",
    "\n",
    "best_params = max(cv_scores, key=lambda x: x[\"mean_accuracy\"]) #find the parameters with the highest (best) cv score\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2107-f4fa-4c61-af90-813694b3f6b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bea24c-a24f-45e3-8162-796580f67686",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract best parameters\n",
    "best_max_iter = best_params[\"max_iter\"]\n",
    "best_hidden_layers = best_params[\"hidden_layer_sizes\"]\n",
    "best_tol = best_params[\"tol\"]\n",
    "best_alpha = best_params[\"alpha\"]\n",
    "\n",
    "# Train final model using best parameters\n",
    "best_model = MLPRegressor(solver='lbfgs', \n",
    "                          max_iter=best_max_iter, \n",
    "                          hidden_layer_sizes=best_hidden_layers, \n",
    "                          tol=best_tol, \n",
    "                          alpha=best_alpha,\n",
    "                          random_state=1)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Final Model - MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Final Model - R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06385a0d-41f3-42a4-92f3-297c608bb852",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "I think we should ask how to make this faster because...it took me like 30 minutes to run all of this or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce58ba-6fc0-474c-8649-97f76b9385c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing, svm \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) \n",
    "linRG = LinearRegression()\n",
    "linRG.fit(X_train, y_train)\n",
    "linRG.predict(X_test)\n",
    "linRG.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ac616-80dc-47e6-9ce0-a2a034f4a20d",
   "metadata": {},
   "source": [
    "Trying new neural network based off of https://www.kaggle.com/code/kelixirr/us-accidents-severity-prediction-end-to-end#Preparing-Our-Data-For-The-Model\n",
    "\n",
    "(i had to download tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5cdb724f-e070-4860-bdaf-6335a67f8e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "import sys\n",
    "# print(\"Python version:\", sys.version)\n",
    "import tensorflow as tf\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "# keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "340995de-b9a6-414b-8319-a6bde99439eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 26161\n",
      "Validation Set Size: 5232\n",
      "Test Set Size: 3489\n"
     ]
    }
   ],
   "source": [
    "# usually only outputs error if y_test has wrong values\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(us_accidents, test_size=0.3)\n",
    "# model to predict relative accident frequency based off of location and time\n",
    "# X, y = prepare_data(df=train, split=True, predictors=['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'], target='Severity')\n",
    "df_final = prepare_data(train, split=False)\n",
    "X = df_final.drop(columns=['Severity'])\n",
    "y = df_final['Severity']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=42, stratify=y_temp)  # 0.4 * 25% = 10%\n",
    "\n",
    "print(f\"Training Set Size: {len(X_train)}\")\n",
    "print(f\"Validation Set Size: {len(X_val)}\")\n",
    "print(f\"Test Set Size: {len(X_test)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "04e3562d-f107-48d4-a947-1b77b71c0b28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4]\n"
     ]
    }
   ],
   "source": [
    "unique_values = y_test.unique()\n",
    "\n",
    "print(unique_values)\n",
    "# should be between [0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e30cc853-58ba-41d5-ae18-26183bb8d99f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train = y_train - 1\n",
    "y_val = y_val - 1  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_scaled, y_val))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "28a103f7-083c-4938-94ac-6199cd3efbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "unique_values = train['Severity'].unique()\n",
    "\n",
    "print(unique_values) # 4 unique class for accident severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "48e07f95-c70e-4a76-b39f-fe7a11969abf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m4,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m132\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,636</span> (193.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49,636\u001b[0m (193.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,676</span> (190.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m48,676\u001b[0m (190.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape = (X_train_scaled.shape[1],)))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(4, activation='softmax'))  # 4 classes for accident severity\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ad20bf72-7930-4655-8f42-837d29a883da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1946/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8349 - loss: 0.5233\n",
      "Epoch 1: val_loss improved from inf to 0.02986, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.8366 - loss: 0.5184 - val_accuracy: 0.9913 - val_loss: 0.0299 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0387\n",
      "Epoch 2: val_loss improved from 0.02986 to 0.02591, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0387 - val_accuracy: 0.9914 - val_loss: 0.0259 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1954/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0324\n",
      "Epoch 3: val_loss improved from 0.02591 to 0.02400, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0324 - val_accuracy: 0.9915 - val_loss: 0.0240 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1952/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0287\n",
      "Epoch 4: val_loss did not improve from 0.02400\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0287 - val_accuracy: 0.9919 - val_loss: 0.0264 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1961/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0274\n",
      "Epoch 5: val_loss did not improve from 0.02400\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0274 - val_accuracy: 0.9923 - val_loss: 0.0251 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1969/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0264\n",
      "Epoch 6: val_loss improved from 0.02400 to 0.02348, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0264 - val_accuracy: 0.9925 - val_loss: 0.0235 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1970/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9919 - loss: 0.0261\n",
      "Epoch 7: val_loss improved from 0.02348 to 0.02242, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9919 - loss: 0.0261 - val_accuracy: 0.9926 - val_loss: 0.0224 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1968/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0252\n",
      "Epoch 8: val_loss improved from 0.02242 to 0.02167, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0252 - val_accuracy: 0.9928 - val_loss: 0.0217 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1960/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0247\n",
      "Epoch 9: val_loss improved from 0.02167 to 0.02086, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9923 - loss: 0.0247 - val_accuracy: 0.9931 - val_loss: 0.0209 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1950/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0225\n",
      "Epoch 10: val_loss improved from 0.02086 to 0.01984, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0225 - val_accuracy: 0.9936 - val_loss: 0.0198 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1974/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0214\n",
      "Epoch 11: val_loss improved from 0.01984 to 0.01881, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0214 - val_accuracy: 0.9939 - val_loss: 0.0188 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1960/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0211\n",
      "Epoch 12: val_loss improved from 0.01881 to 0.01865, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0211 - val_accuracy: 0.9941 - val_loss: 0.0187 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1973/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0208\n",
      "Epoch 13: val_loss improved from 0.01865 to 0.01857, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0208 - val_accuracy: 0.9942 - val_loss: 0.0186 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1960/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0206\n",
      "Epoch 14: val_loss did not improve from 0.01857\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0206 - val_accuracy: 0.9941 - val_loss: 0.0187 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1966/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0208\n",
      "Epoch 15: val_loss improved from 0.01857 to 0.01855, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0208 - val_accuracy: 0.9943 - val_loss: 0.0185 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1972/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0204\n",
      "Epoch 16: val_loss did not improve from 0.01855\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9935 - loss: 0.0204 - val_accuracy: 0.9943 - val_loss: 0.0200 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9934 - loss: 0.0203\n",
      "Epoch 17: val_loss improved from 0.01855 to 0.01731, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9934 - loss: 0.0203 - val_accuracy: 0.9943 - val_loss: 0.0173 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1963/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0197\n",
      "Epoch 18: val_loss improved from 0.01731 to 0.01723, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0197 - val_accuracy: 0.9941 - val_loss: 0.0172 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1964/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0206\n",
      "Epoch 19: val_loss did not improve from 0.01723\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0206 - val_accuracy: 0.9940 - val_loss: 0.0178 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1954/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0192\n",
      "Epoch 20: val_loss improved from 0.01723 to 0.01698, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9934 - loss: 0.0192 - val_accuracy: 0.9944 - val_loss: 0.0170 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1953/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0185\n",
      "Epoch 21: val_loss did not improve from 0.01698\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0185 - val_accuracy: 0.9943 - val_loss: 0.0173 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1974/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0187\n",
      "Epoch 22: val_loss improved from 0.01698 to 0.01673, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0187 - val_accuracy: 0.9945 - val_loss: 0.0167 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1960/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0179\n",
      "Epoch 23: val_loss improved from 0.01673 to 0.01607, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0179 - val_accuracy: 0.9945 - val_loss: 0.0161 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1969/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0186\n",
      "Epoch 24: val_loss did not improve from 0.01607\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0186 - val_accuracy: 0.9945 - val_loss: 0.0163 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1948/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9938 - loss: 0.0176\n",
      "Epoch 25: val_loss did not improve from 0.01607\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9938 - loss: 0.0176 - val_accuracy: 0.9946 - val_loss: 0.0165 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1969/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0175\n",
      "Epoch 26: val_loss did not improve from 0.01607\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0175 - val_accuracy: 0.9946 - val_loss: 0.0173 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1966/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0178\n",
      "Epoch 27: val_loss improved from 0.01607 to 0.01593, saving model to best_model.keras\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0178 - val_accuracy: 0.9946 - val_loss: 0.0159 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1954/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0183\n",
      "Epoch 28: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0183 - val_accuracy: 0.9948 - val_loss: 0.0167 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1969/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0178\n",
      "Epoch 29: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0178 - val_accuracy: 0.9949 - val_loss: 0.0160 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1956/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0174\n",
      "Epoch 30: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0174 - val_accuracy: 0.9948 - val_loss: 0.0160 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1972/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0175\n",
      "Epoch 31: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0175 - val_accuracy: 0.9948 - val_loss: 0.0164 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1953/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0173\n",
      "Epoch 32: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9943 - loss: 0.0173 - val_accuracy: 0.9948 - val_loss: 0.0163 - learning_rate: 1.2500e-04\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * (drop ** np.floor((1+epoch)/epochs_drop))\n",
    "    return lr\n",
    "\n",
    "checkpoint_path = 'best_model.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=50,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[checkpoint, early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6f493a75-abe7-40ff-82ec-a1847ec20e26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile('best_model.keras') # make sure true before loading saved models in later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d640c8a1-ec29-49db-a794-842bb07c2071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n",
      "\u001b[1m1974/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0173\n",
      "Epoch 51: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0173 - val_accuracy: 0.9947 - val_loss: 0.0167 - learning_rate: 3.1250e-05\n",
      "Epoch 52/150\n",
      "\u001b[1m1959/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0168\n",
      "Epoch 52: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0168 - val_accuracy: 0.9948 - val_loss: 0.0166 - learning_rate: 3.1250e-05\n",
      "Epoch 53/150\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0169\n",
      "Epoch 53: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0169 - val_accuracy: 0.9948 - val_loss: 0.0169 - learning_rate: 3.1250e-05\n",
      "Epoch 54/150\n",
      "\u001b[1m1954/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0169\n",
      "Epoch 54: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0169 - val_accuracy: 0.9948 - val_loss: 0.0163 - learning_rate: 3.1250e-05\n",
      "Epoch 55/150\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0167\n",
      "Epoch 55: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0167 - val_accuracy: 0.9950 - val_loss: 0.0165 - learning_rate: 3.1250e-05\n",
      "Epoch 56/150\n",
      "\u001b[1m1965/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0173\n",
      "Epoch 56: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0173 - val_accuracy: 0.9950 - val_loss: 0.0163 - learning_rate: 3.1250e-05\n",
      "Epoch 57/150\n",
      "\u001b[1m1956/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0168\n",
      "Epoch 57: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0168 - val_accuracy: 0.9950 - val_loss: 0.0164 - learning_rate: 3.1250e-05\n",
      "Epoch 58/150\n",
      "\u001b[1m1957/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0166\n",
      "Epoch 58: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0166 - val_accuracy: 0.9949 - val_loss: 0.0161 - learning_rate: 3.1250e-05\n",
      "Epoch 59/150\n",
      "\u001b[1m1974/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0161\n",
      "Epoch 59: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0161 - val_accuracy: 0.9949 - val_loss: 0.0163 - learning_rate: 3.1250e-05\n",
      "Epoch 60/150\n",
      "\u001b[1m1957/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0161\n",
      "Epoch 60: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0161 - val_accuracy: 0.9950 - val_loss: 0.0165 - learning_rate: 1.5625e-05\n",
      "Epoch 61/150\n",
      "\u001b[1m1970/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0161\n",
      "Epoch 61: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0161 - val_accuracy: 0.9950 - val_loss: 0.0164 - learning_rate: 1.5625e-05\n",
      "Epoch 62/150\n",
      "\u001b[1m1950/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0161\n",
      "Epoch 62: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0161 - val_accuracy: 0.9951 - val_loss: 0.0161 - learning_rate: 1.5625e-05\n",
      "Epoch 63/150\n",
      "\u001b[1m1951/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0161\n",
      "Epoch 63: val_loss did not improve from 0.01593\n",
      "\u001b[1m1975/1975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0161 - val_accuracy: 0.9951 - val_loss: 0.0164 - learning_rate: 1.5625e-05\n",
      "Epoch 63: early stopping\n",
      "Restoring model weights from the end of the best epoch: 58.\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model('/kaggle/working/best_model.keras')\n",
    "model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    initial_epoch=50, \n",
    "    epochs=150,\n",
    "    callbacks=[checkpoint, early_stopping, lr_scheduler],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "846333cf-e4b0-48a4-91ab-57611902f486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9943 - loss: 0.0168\n",
      "Validation Accuracy: 0.9946177005767822\n"
     ]
    }
   ],
   "source": [
    "nn_model = tf.keras.models.load_model(\"best_model.keras\")\n",
    "val_loss, val_accuracy = nn_model.evaluate(val_dataset) \n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d3908791-e760-44e6-a469-b394bf096eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4811478    1\n",
      "6254359    1\n",
      "4043068    1\n",
      "4873285    1\n",
      "5232055    1\n",
      "          ..\n",
      "4299844    1\n",
      "4780388    1\n",
      "4406789    1\n",
      "5612832    1\n",
      "5612758    1\n",
      "Name: Severity, Length: 3489, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test - 1\n",
    "print(y_test) # why are y_test values < 0\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test)) \n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fbe44a4b-1b5d-476a-bc51-8baba132315d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0301  \n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = nn_model.evaluate(test_dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cf60dd1f-464d-4af4-88e3-759cdaf55b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      3475\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           1.00      3489\n",
      "   macro avg       0.33      0.33      0.33      3489\n",
      "weighted avg       0.99      1.00      0.99      3489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\littl\\anaconda3\\envs\\PIC16B-25W\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\littl\\anaconda3\\envs\\PIC16B-25W\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\littl\\anaconda3\\envs\\PIC16B-25W\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\littl\\anaconda3\\envs\\PIC16B-25W\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\littl\\anaconda3\\envs\\PIC16B-25W\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\littl\\anaconda3\\envs\\PIC16B-25W\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = np.argmax(nn_model.predict(X_test_scaled), axis=1) \n",
    "# y_pred = np.argmax(model.predict(X_test_scaled), axis=1) \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cf24b-9fa7-4caa-9e3e-a0186d025370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B-25W] *",
   "language": "python",
   "name": "conda-env-PIC16B-25W-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
