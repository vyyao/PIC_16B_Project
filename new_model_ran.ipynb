{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd1e38a-90bd-45ea-9451-be526e8ce768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import express as px\n",
    "import plotly.graph_objs as go\n",
    "# ML libraries - idk which one to use yet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, mean_absolute_error, r2_score\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import cKDTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92a59d-4641-42da-b466-fd1c2c11ad8b",
   "metadata": {},
   "source": [
    "**where i left off (hailey)**\n",
    "- TA said to use supervised neural network instead of linear regression so I chose MLPRegressor but its lowkey ass (see below)\n",
    "- I was thinking predicting relative risk at location/hour/weather would be ideal but idk bc the model isn't looking too hot (this would also require requesting hourly nasa data which i didn't fix get_nasa_power_data to change date to hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedc2883-6e80-41c9-aec4-5f6d35eb4648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NASA API\n",
    "def get_nasa_power_data(lat, lon, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches NASA POWER API data for given latitude, longitude, and time range.\n",
    "\n",
    "    Args:\n",
    "    - lat (float): Latitude of the location.\n",
    "    - lon (float): Longitude of the location.\n",
    "    - start_date (str): Start date in YYYYMMDD format.\n",
    "    - end_date (str): End date in YYYYMMDD format.\n",
    "\n",
    "    Returns:\n",
    "    - Pandas DataFrame with selected weather parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Specify multiple parameters in the API request\n",
    "    parameters = \"PRECSNO,T2MDEW,PRECTOTCORR,T2M,WS2M\"\n",
    "\n",
    "    url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "    params = {\n",
    "        \"parameters\": parameters,\n",
    "        \"community\": \"RE\",\n",
    "        \"longitude\": lon,\n",
    "        \"latitude\": lat,\n",
    "        \"start\": start_date,\n",
    "        \"end\": end_date,\n",
    "        \"format\": \"JSON\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # Convert JSON response to DataFrame and transpose it\n",
    "    nasa_weather = pd.DataFrame.from_dict(data[\"properties\"][\"parameter\"], orient=\"index\").T\n",
    "\n",
    "    # Reset index and rename date column\n",
    "    nasa_weather.reset_index(inplace=True)\n",
    "    nasa_weather.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "\n",
    "    # Convert date column to proper datetime format\n",
    "    nasa_weather[\"date\"] = pd.to_datetime(nasa_weather[\"date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    nasa_weather.dropna(subset=[\"date\"], inplace=True)  # Remove invalid date rows\n",
    "\n",
    "    nasa_weather.rename(columns={\n",
    "        \"PRECSNO\": \"Snow_Precipitation\",\n",
    "        \"T2MDEW\": \"Dew_Point_2m\",\n",
    "        \"PRECTOTCORR\": \"Total_Precipitation_mm\",\n",
    "        \"T2M\": \"Temperature_2m_C\",\n",
    "        \"WS2M\": \"Wind_Speed_2m\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add Rounded_Lat and Rounded_Lng for merging\n",
    "    nasa_weather['Rounded_Lat'] = lat\n",
    "    nasa_weather['Rounded_Lng'] = lon\n",
    "    \n",
    "    # Display DataFrame\n",
    "    print(f\"\\n Weather Data for Latitude {lat}, Longitude {lon}\\n\")\n",
    "    print(f\"\\n Weather Data for Latitude {lat}, Longitude {lon}\\n\")\n",
    "    nasa_weather['Precipitation(in)'] = nasa_weather['Total_Precipitation_mm'] / 25.4 # mm to in\n",
    "    nasa_weather['Temperature(F)'] = (nasa_weather['Temperature_2m_C'] * (9./5.)) + 32. # C to F\n",
    "    nasa_weather['Wind_Speed(mph)'] = nasa_weather['Wind_Speed_2m'] * 2.237 # m/s to mph\n",
    "    # nasa_weather.dropna()\n",
    "    display(nasa_weather)  # Works in Jupyter Notebook\n",
    "\n",
    "    return nasa_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9938d54-cead-4cf2-a65f-b238d91be651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weather Data for Latitude 34.05, Longitude -118.25\n",
      "\n",
      "\n",
      " Weather Data for Latitude 34.05, Longitude -118.25\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Snow_Precipitation</th>\n",
       "      <th>Dew_Point_2m</th>\n",
       "      <th>Total_Precipitation_mm</th>\n",
       "      <th>Temperature_2m_C</th>\n",
       "      <th>Wind_Speed_2m</th>\n",
       "      <th>Rounded_Lat</th>\n",
       "      <th>Rounded_Lng</th>\n",
       "      <th>Precipitation(in)</th>\n",
       "      <th>Temperature(F)</th>\n",
       "      <th>Wind_Speed(mph)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>12.14</td>\n",
       "      <td>1.64</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>53.852</td>\n",
       "      <td>3.66868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>11.55</td>\n",
       "      <td>1.59</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>52.790</td>\n",
       "      <td>3.55683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>5.87</td>\n",
       "      <td>10.86</td>\n",
       "      <td>3.25</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.231102</td>\n",
       "      <td>51.548</td>\n",
       "      <td>7.27025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9.76</td>\n",
       "      <td>2.89</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>49.568</td>\n",
       "      <td>6.46493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.440</td>\n",
       "      <td>4.47400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10.50</td>\n",
       "      <td>2.86</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>50.900</td>\n",
       "      <td>6.39782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>7.77</td>\n",
       "      <td>5.29</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>45.986</td>\n",
       "      <td>11.83373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Snow_Precipitation  Dew_Point_2m  Total_Precipitation_mm  \\\n",
       "0 2024-01-01                 0.0          6.12                    0.04   \n",
       "1 2024-01-02                 0.0          7.05                    0.09   \n",
       "2 2024-01-03                 0.0          6.75                    5.87   \n",
       "3 2024-01-04                 0.0          2.14                    0.02   \n",
       "4 2024-01-05                 0.0          1.99                    0.00   \n",
       "5 2024-01-06                 0.0          1.23                    0.12   \n",
       "6 2024-01-07                 0.0         -0.09                    0.26   \n",
       "\n",
       "   Temperature_2m_C  Wind_Speed_2m  Rounded_Lat  Rounded_Lng  \\\n",
       "0             12.14           1.64        34.05      -118.25   \n",
       "1             11.55           1.59        34.05      -118.25   \n",
       "2             10.86           3.25        34.05      -118.25   \n",
       "3              9.76           2.89        34.05      -118.25   \n",
       "4             10.80           2.00        34.05      -118.25   \n",
       "5             10.50           2.86        34.05      -118.25   \n",
       "6              7.77           5.29        34.05      -118.25   \n",
       "\n",
       "   Precipitation(in)  Temperature(F)  Wind_Speed(mph)  \n",
       "0           0.001575          53.852          3.66868  \n",
       "1           0.003543          52.790          3.55683  \n",
       "2           0.231102          51.548          7.27025  \n",
       "3           0.000787          49.568          6.46493  \n",
       "4           0.000000          51.440          4.47400  \n",
       "5           0.004724          50.900          6.39782  \n",
       "6           0.010236          45.986         11.83373  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Fetch data for different locations\n",
    "df_la = get_nasa_power_data(34.05, -118.25, \"20240101\", \"20240107\")  # Los Angeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efa465c-3a70-4b5f-bf78-ccc26b0b5fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lng</th>\n",
       "      <th>Distance(mi)</th>\n",
       "      <th>...</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Civil_Twilight</th>\n",
       "      <th>Nautical_Twilight</th>\n",
       "      <th>Astronomical_Twilight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-1</td>\n",
       "      <td>Source2</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 05:46:00</td>\n",
       "      <td>2016-02-08 11:00:00</td>\n",
       "      <td>39.865147</td>\n",
       "      <td>-84.058723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-2</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:07:59</td>\n",
       "      <td>2016-02-08 06:37:59</td>\n",
       "      <td>39.928059</td>\n",
       "      <td>-82.831184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-3</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:49:27</td>\n",
       "      <td>2016-02-08 07:19:27</td>\n",
       "      <td>39.063148</td>\n",
       "      <td>-84.032608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-4</td>\n",
       "      <td>Source2</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 07:23:34</td>\n",
       "      <td>2016-02-08 07:53:34</td>\n",
       "      <td>39.747753</td>\n",
       "      <td>-84.205582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-5</td>\n",
       "      <td>Source2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 07:39:07</td>\n",
       "      <td>2016-02-08 08:09:07</td>\n",
       "      <td>39.627781</td>\n",
       "      <td>-84.188354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Source  Severity           Start_Time             End_Time  \\\n",
       "0  A-1  Source2         3  2016-02-08 05:46:00  2016-02-08 11:00:00   \n",
       "1  A-2  Source2         2  2016-02-08 06:07:59  2016-02-08 06:37:59   \n",
       "2  A-3  Source2         2  2016-02-08 06:49:27  2016-02-08 07:19:27   \n",
       "3  A-4  Source2         3  2016-02-08 07:23:34  2016-02-08 07:53:34   \n",
       "4  A-5  Source2         2  2016-02-08 07:39:07  2016-02-08 08:09:07   \n",
       "\n",
       "   Start_Lat  Start_Lng  End_Lat  End_Lng  Distance(mi)  ... Roundabout  \\\n",
       "0  39.865147 -84.058723      NaN      NaN          0.01  ...      False   \n",
       "1  39.928059 -82.831184      NaN      NaN          0.01  ...      False   \n",
       "2  39.063148 -84.032608      NaN      NaN          0.01  ...      False   \n",
       "3  39.747753 -84.205582      NaN      NaN          0.01  ...      False   \n",
       "4  39.627781 -84.188354      NaN      NaN          0.01  ...      False   \n",
       "\n",
       "  Station   Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset  \\\n",
       "0   False  False           False          False        False          Night   \n",
       "1   False  False           False          False        False          Night   \n",
       "2   False  False           False           True        False          Night   \n",
       "3   False  False           False          False        False          Night   \n",
       "4   False  False           False           True        False            Day   \n",
       "\n",
       "  Civil_Twilight Nautical_Twilight Astronomical_Twilight  \n",
       "0          Night             Night                 Night  \n",
       "1          Night             Night                   Day  \n",
       "2          Night               Day                   Day  \n",
       "3            Day               Day                   Day  \n",
       "4            Day               Day                   Day  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect us_accident data\n",
    "us_accidents = pd.read_csv('US_Accidents_March23.csv')\n",
    "us_accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a74597-2201-4f2c-9b95-cbba4120d711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Light Rain' 'Overcast' 'Mostly Cloudy' 'Rain' 'Light Snow' 'Haze'\n",
      " 'Scattered Clouds' 'Partly Cloudy' 'Clear' 'Snow'\n",
      " 'Light Freezing Drizzle' 'Light Drizzle' 'Fog' 'Shallow Fog' 'Heavy Rain'\n",
      " 'Light Freezing Rain' 'Cloudy' 'Drizzle' nan 'Light Rain Showers' 'Mist'\n",
      " 'Smoke' 'Patches of Fog' 'Light Freezing Fog' 'Light Haze'\n",
      " 'Light Thunderstorms and Rain' 'Thunderstorms and Rain' 'Fair'\n",
      " 'Volcanic Ash' 'Blowing Sand' 'Blowing Dust / Windy' 'Widespread Dust'\n",
      " 'Fair / Windy' 'Rain Showers' 'Mostly Cloudy / Windy'\n",
      " 'Light Rain / Windy' 'Hail' 'Heavy Drizzle' 'Showers in the Vicinity'\n",
      " 'Thunderstorm' 'Light Rain Shower' 'Light Rain with Thunder'\n",
      " 'Partly Cloudy / Windy' 'Thunder in the Vicinity' 'T-Storm'\n",
      " 'Heavy Thunderstorms and Rain' 'Thunder' 'Heavy T-Storm' 'Funnel Cloud'\n",
      " 'Heavy T-Storm / Windy' 'Blowing Snow' 'Light Thunderstorms and Snow'\n",
      " 'Heavy Snow' 'Low Drifting Snow' 'Light Ice Pellets' 'Ice Pellets'\n",
      " 'Squalls' 'N/A Precipitation' 'Cloudy / Windy' 'Light Fog' 'Sand'\n",
      " 'Snow Grains' 'Snow Showers' 'Heavy Thunderstorms and Snow'\n",
      " 'Rain / Windy' 'Heavy Rain / Windy' 'Heavy Ice Pellets'\n",
      " 'Light Snow / Windy' 'Heavy Freezing Rain' 'Small Hail'\n",
      " 'Heavy Rain Showers' 'Thunder / Windy' 'Drizzle and Fog'\n",
      " 'T-Storm / Windy' 'Blowing Dust' 'Smoke / Windy' 'Haze / Windy' 'Tornado'\n",
      " 'Light Drizzle / Windy' 'Widespread Dust / Windy' 'Wintry Mix'\n",
      " 'Wintry Mix / Windy' 'Light Snow with Thunder' 'Fog / Windy'\n",
      " 'Snow and Thunder' 'Light Snow Shower' 'Sleet' 'Light Snow and Sleet'\n",
      " 'Snow / Windy' 'Rain Shower' 'Snow and Sleet' 'Light Sleet'\n",
      " 'Heavy Snow / Windy' 'Freezing Drizzle' 'Light Freezing Rain / Windy'\n",
      " 'Thunder / Wintry Mix' 'Blowing Snow / Windy' 'Freezing Rain'\n",
      " 'Light Snow and Sleet / Windy' 'Snow and Sleet / Windy' 'Sleet / Windy'\n",
      " 'Heavy Freezing Rain / Windy' 'Squalls / Windy'\n",
      " 'Light Rain Shower / Windy' 'Snow and Thunder / Windy'\n",
      " 'Light Sleet / Windy' 'Sand / Dust Whirlwinds' 'Mist / Windy'\n",
      " 'Drizzle / Windy' 'Duststorm' 'Sand / Dust Whirls Nearby'\n",
      " 'Thunder and Hail' 'Heavy Sleet' 'Freezing Rain / Windy'\n",
      " 'Light Snow Shower / Windy' 'Partial Fog' 'Thunder / Wintry Mix / Windy'\n",
      " 'Patches of Fog / Windy' 'Rain and Sleet' 'Light Snow Grains'\n",
      " 'Partial Fog / Windy' 'Sand / Dust Whirlwinds / Windy'\n",
      " 'Heavy Snow with Thunder' 'Light Snow Showers' 'Heavy Blowing Snow'\n",
      " 'Light Hail' 'Heavy Smoke' 'Heavy Thunderstorms with Small Hail'\n",
      " 'Light Thunderstorm' 'Heavy Freezing Drizzle' 'Light Blowing Snow'\n",
      " 'Thunderstorms and Snow' 'Dust Whirls' 'Rain Shower / Windy'\n",
      " 'Sleet and Thunder' 'Heavy Sleet and Thunder' 'Drifting Snow / Windy'\n",
      " 'Shallow Fog / Windy' 'Thunder and Hail / Windy' 'Heavy Sleet / Windy'\n",
      " 'Sand / Windy' 'Heavy Rain Shower / Windy' 'Blowing Snow Nearby'\n",
      " 'Heavy Rain Shower' 'Drifting Snow']\n"
     ]
    }
   ],
   "source": [
    "unique_values = us_accidents['Weather_Condition'].unique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e57d6c3-abd1-4744-a871-1e401bf3aee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of all columns in US Accident Data\n",
    "drop_cols = ['ID',\n",
    "            'Source',\n",
    "            # 'Severity', # Severity = target column, 1-4, where 1 indicates the least impact on traffic\n",
    "            'Start_Time',\n",
    "            'End_Time',\n",
    "            'Start_Lat',  \n",
    "            'Start_Lng', \n",
    "            'End_Lat',\n",
    "            'End_Lng',\n",
    "            'Distance(mi)', # Distance(mi) = target column?, length of road extent affected by accident in miles\n",
    "            'Description', # Description = human description of accident\n",
    "            'Street', \n",
    "            'City', \n",
    "            'County',\n",
    "            'State',\n",
    "            'Zipcode',\n",
    "            'Country',\n",
    "            'Timezone',\n",
    "            'Airport_Code',\n",
    "            'Weather_Timestamp', # Weather_Timestamp = shows time-stamp of weather observation record (in local time)\n",
    "            # 'Temperature(F)',\n",
    "            'Wind_Chill(F)',\n",
    "            'Humidity(%)',\n",
    "            'Pressure(in)',\n",
    "            'Visibility(mi)',\n",
    "            'Wind_Direction',\n",
    "            # 'Wind_Speed(mph)',\n",
    "            # 'Precipitation(in)',\n",
    "            # 'Weather_Condition',\n",
    "            'Amenity',\n",
    "            'Bump',\n",
    "            'Crossing',\n",
    "            'Give_Way',\n",
    "            'Junction',\n",
    "            'No_Exit',\n",
    "            'Railway',\n",
    "            'Roundabout',\n",
    "            'Station',\n",
    "            'Stop',\n",
    "            'Traffic_Calming',\n",
    "            'Traffic_Signal',\n",
    "            'Turning_Loop',\n",
    "            'Sunrise_Sunset', # day or night based on sunrise/sunset\n",
    "            'Civil_Twilight', # day or night based on civil twilight\n",
    "            'Nautical_Twilight', # day or night based on nautical twilight\n",
    "            'Astronomical_Twilight'] # day or night based on astronomical twilight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d4b6a1-e2da-40a3-939c-10d46e3eca18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# string feature that if kept will need to be encoded for ML\n",
    "str_features = 'Weather_Condition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392196e6-5ad4-4765-987d-ff54fe162e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, split, predictors, target):\n",
    "    \"\"\"\n",
    "    Prepares the US Accidents DataFrame for merging with NASA weather data, keeping only necessary columns.\n",
    "    \n",
    "    Args:\n",
    "    - df (DataFrame): Raw US Accidents dataset.\n",
    "    - split (boolean): if true, split df by target and predictor data\n",
    "    - ml_drop (list): other columns to drop for machine learning model (adjustable if we decide a variable is not good at predicting)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame with 'date', 'Rounded_Lat', 'Rounded_Lng', and 'Severity' columns.\n",
    "    \"\"\"\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Convert time columns to datetime format\n",
    "    df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "\n",
    "    # Remove rows with invalid 'Start_Time' values\n",
    "    df = df[df['Start_Time'].notnull()].copy()\n",
    "\n",
    "    # Extract 'date' from 'Start_Time' for merging with NASA weather data\n",
    "    # df['date'] = df['Start_Time'].dt.date\n",
    "\n",
    "    # Filter for coordinates within LA County\n",
    "    df = df[(df['Start_Lat'].between(33.7, 34.8)) & (df['Start_Lng'].between(-119.0, -117.6))]\n",
    "\n",
    "    # Round latitude and longitude to 2 decimal places for approximate matching\n",
    "    df['Rounded_Lat'] = df['Start_Lat'].round(2)\n",
    "    df['Rounded_Lng'] = df['Start_Lng'].round(2)\n",
    "    \n",
    "    # encode str_features\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['Weather_Condition'] = le.fit_transform(df['Weather_Condition'])\n",
    "    \n",
    "    # add new columns for increased risk with associated conditions and time\n",
    "    # for now will start by increased risk with associated weather and time\n",
    "    df['hour'] = df['Start_Time'].astype(str).str.split(' ').str.get(1).str.split(':').str.get(0)\n",
    "    \n",
    "    # INCREASED SEVERITY RISK BASED ON LOCATION AND TIME  --------------------------------------------------------------------------\n",
    "    \n",
    "    # add col to represent frequency of accidents at this location and time\n",
    "    df['lt_frequency'] = df.groupby(['Rounded_Lat','Rounded_Lng','hour'])['Severity'].transform('count')\n",
    "\n",
    "    # add col to represent frequency rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lt_frequency_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','hour'])['lt_frequency'].rank()\n",
    "\n",
    "    # add col to represent severity rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lt_severity_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','hour'])['Severity'].rank()\n",
    "    \n",
    "    # INCREASED SEVERITY RISK BASED ON LOCATION AND WEATHER  --------------------------------------------------------------------------\n",
    "    \n",
    "    # add col to represent frequency of accidents at this location and time\n",
    "    df['lw_frequency'] = df.groupby(['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'])['Severity'].transform('count')\n",
    "\n",
    "    # add col to represent frequency rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lw_frequency_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'])['lw_frequency'].rank()\n",
    "\n",
    "    # add col to represent severity rank of accidents at this location and time compared to rest of location and times\n",
    "    # rank 0 = lowest frequency rank\n",
    "    df['lw_severity_rank'] = df.groupby(['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'])['Severity'].rank()\n",
    "    \n",
    "    # Compute Risk Changes-----------------------------------------------------------------------------------\n",
    "    \n",
    "    # Average accident frequency per hour (time baseline)\n",
    "    avg_hourly_accidents = df.groupby('hour')['lt_frequency'].transform('mean')\n",
    "    df['time_risk_change'] = (df['lt_frequency'] - avg_hourly_accidents) / avg_hourly_accidents\n",
    "\n",
    "    # Average accident frequency per weather condition (weather baseline)\n",
    "    avg_weather_accidents = df.groupby('Weather_Condition')['lt_frequency'].transform('mean')\n",
    "    df['weather_risk_change'] = (df['lt_frequency'] - avg_weather_accidents) / avg_weather_accidents\n",
    "\n",
    "    # Combined risk factor (balancing time & weather risks)\n",
    "    df['combined_risk'] = 0.5 * df['time_risk_change'] + 0.5 * df['weather_risk_change']\n",
    "\n",
    "    # drop NaN\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # for machine learning clean and prep\n",
    "    if split:\n",
    "        X=df[predictors]\n",
    "        y=df[target]\n",
    "        return X,y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674161b2-3e36-4538-9c19-ad77a11526ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - MAE: 0.6178865755651441\n",
      "Linear Regression - R²: 0.031090321714503655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "train, test = train_test_split(us_accidents, test_size=0.3)\n",
    "\n",
    "# Define predictors and target\n",
    "predictors = ['Rounded_Lat', 'Rounded_Lng', 'hour', 'Weather_Condition']\n",
    "target = 'combined_risk'  # Predicting overall accident risk\n",
    "\n",
    "# Prepare data\n",
    "X, y = prepare_data(train, split=True, predictors=predictors, target=target)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Linear Regression - MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Linear Regression - R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9bd268-6fe5-48b1-a209-675c003d028a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized SGD Regression - MAE: 0.617468498728528\n",
      "Optimized SGD Regression - R²: 0.03092673291672321\n",
      "CPU times: user 31.8 s, sys: 31.1 s, total: 1min 2s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(us_accidents, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define predictors and target\n",
    "predictors = ['Rounded_Lat', 'Rounded_Lng', 'hour', 'Weather_Condition']\n",
    "target = 'combined_risk'\n",
    "\n",
    "# Prepare data\n",
    "X, y = prepare_data(train, split=True, predictors=predictors, target=target)\n",
    "\n",
    "# Convert to float32 for efficiency\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features using faster StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Use SGDRegressor (much faster for large datasets)\n",
    "sgd_model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "sgd_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = sgd_model.predict(X_test_scaled)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Optimized SGD Regression - MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Optimized SGD Regression - R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc0d5f1-2034-40b5-9579-fb7bddf73b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression - MAE: 0.5826143\n",
      "Polynomial Regression - R²: 0.12081077959833675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Use polynomial regression (degree=2)\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "poly_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_poly = poly_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Polynomial Regression - MAE:\", mean_absolute_error(y_test, y_pred_poly))\n",
    "print(\"Polynomial Regression - R²:\", r2_score(y_test, y_pred_poly))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e76987-71ca-417e-a022-59014571d039",
   "metadata": {},
   "source": [
    "### MLPRegressor\n",
    "\n",
    "- sklearn documentation https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "- lowkey don't know good risk metric\n",
    "- what do we want to predict?\n",
    "- we could have 2 different models that relative accident frequency based off of location and weather and location and time (hour) --> predicts lw_frequency_rank and lt_frequency_rank (i think predicting relative accident frequency is more intuitive than relative accident severity)\n",
    "- we could have 2 different models that relative accident severity based off of location and weather and location and time (hour) --> predicts lw_severity_rank and lt_severity_rank\n",
    "- we could have 4 different models that do both above ^\n",
    "- we could come up with a new metric that combines all and only have one model (i just don't know best way to represent that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67570ed0-ae98-46eb-83e0-1fc7fe66b022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(us_accidents, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63246330-537e-4ec8-8042-3051bc998831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model to predict relative accident frequency based off of location and time\n",
    "X, y = prepare_data(df=train, split=True, predictors=['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)'], target='lw_frequency_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f0a9073-c7fb-4564-9529-870686750d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07141754688896529"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also takes forever\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "lt_regr = MLPRegressor(random_state=1, solver='lbfgs',max_iter=3000, tol=0.02, hidden_layer_sizes=200, alpha=0.005, )\n",
    "lt_regr.fit(X_train, y_train)\n",
    "lt_regr.predict(X_test)\n",
    "lt_regr.score(X_test, y_test) # sad scores :(\n",
    "# increasing max_iter seems to increase the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca0394-21e7-4bf2-b99b-15439339542c",
   "metadata": {},
   "source": [
    "idk... i tried to make it faster by only picking 10 sets of parameters instead of testing everything and not using grid search because that may be making it slower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4f2367f-fb6c-4454-8b4d-a643f23b66a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(41869) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41871) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41872) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41873) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41874) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41875) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41876) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41877) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41878) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(41879) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'tol': 0.2, 'max_iter': 2000, 'hidden_layer_sizes': (300,), 'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Scale features to normilize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define model, settings optimize speed\n",
    "mlp = MLPRegressor(solver='adam', \n",
    "                   activation='relu', \n",
    "                   batch_size=128, \n",
    "                   early_stopping=True, # prevents unnecessary training\n",
    "                   random_state=1)\n",
    "\n",
    "# Define parameter space\n",
    "param_dist = {\n",
    "    \"max_iter\": [1000, 2000, 3000],\n",
    "    \"hidden_layer_sizes\": [(100,), (200,), (300,)],\n",
    "    \"alpha\": [0.0001, 0.001, 0.005],\n",
    "    \"tol\": [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a7442-6d82-4fc0-8b0a-ad26ecdc0a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ideally once best parameters are found and target prediction is confirmed we just predict whatever using nasa power data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce16ecd-2804-4855-9770-33d12ab3ffdf",
   "metadata": {},
   "source": [
    "Accident frequency depends more on time and location (e.g., rush hour, high-traffic areas), while severity is influenced by weather conditions and road characteristics (e.g., rain, fog, wind speed). By using two separate models, one for frequency and one for severity, we can train each model with the most relevant features, leading to more accurate predictions.\n",
    "\n",
    "But this also takes forever to run so im not sure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d8ca0fd-6876-4c52-8a3c-7e8fd4160bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Model MAE: 2.1558020355122363\n",
      "Frequency Model score: 0.2557353122833105\n",
      "Severity Model - MAE: 1.0836724496461172\n",
      "Severity Model - R²: 0.2569361485259074\n"
     ]
    }
   ],
   "source": [
    "# MODEL 1: Predicting Accident Frequency\n",
    "# learns from past accident data to preduct how frequently accidents occur at a given\n",
    "# time and location\n",
    "# select input features and target\n",
    "predictors_freq = ['Rounded_Lat', 'Rounded_Lng', 'hour']\n",
    "target_freq = 'lt_frequency'\n",
    "\n",
    "# Prepare the data\n",
    "X_freq, y_freq = prepare_data(train, split=True, predictors=predictors_freq, target=target_freq)\n",
    "\n",
    "#Split into training and test sets\n",
    "X_train_freq, X_test_freq, y_train_freq, y_test_freq = train_test_split(X_freq, y_freq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_freq = MinMaxScaler()\n",
    "X_train_freq_scaled = scaler_freq.fit_transform(X_train_freq)\n",
    "X_test_freq_scaled = scaler_freq.transform(X_test_freq)\n",
    "\n",
    "# Train Neural Network to predict accident frequency\n",
    "freq_model = MLPRegressor(hidden_layer_sizes=(64, 32), \n",
    "                          max_iter=2000, solver='adam', \n",
    "                          activation='relu', \n",
    "                          batch_size=256, \n",
    "                          early_stopping=True, \n",
    "                          random_state=1)\n",
    "\n",
    "# Train the model\n",
    "freq_model.fit(X_train_freq_scaled, y_train_freq)\n",
    "\n",
    "# Test on unseen Data\n",
    "y_pred_freq = freq_model.predict(X_test_freq_scaled)\n",
    "\n",
    "# Print Accuracy Results\n",
    "print(\"Frequency Model MAE:\", mean_absolute_error(y_test_freq, y_pred_freq))\n",
    "print(\"Frequency Model score:\", r2_score(y_test_freq, y_pred_freq))\n",
    "\n",
    "# MODEL 2: Predicting Accident Severity\n",
    "# select features and target for severity model\n",
    "predictors_sev = ['Rounded_Lat', 'Rounded_Lng', 'hour']\n",
    "target_sev = 'lt_severity_rank'\n",
    "\n",
    "# Prepare data\n",
    "X_sev, y_sev = prepare_data(train, split=True, predictors=predictors_sev, target=target_sev)\n",
    "\n",
    "# Split into training and testing set\n",
    "X_train_sev, X_test_sev, y_train_sev, y_test_sev = train_test_split(X_sev, y_sev, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_sev = MinMaxScaler()\n",
    "X_train_sev_scaled = scaler_sev.fit_transform(X_train_sev)\n",
    "X_test_sev_scaled = scaler_sev.transform(X_test_sev)\n",
    "\n",
    "# Train Neural Network for Severity Prediction\n",
    "sev_model = MLPRegressor(hidden_layer_sizes=(64, 32), \n",
    "                         max_iter=2000, \n",
    "                         solver='adam', \n",
    "                         activation='relu', \n",
    "                         batch_size=256, \n",
    "                         early_stopping=True, \n",
    "                         random_state=1)\n",
    "\n",
    "# Train model\n",
    "sev_model.fit(X_train_sev_scaled, y_train_sev)\n",
    "\n",
    "# Test model\n",
    "y_pred_sev = sev_model.predict(X_test_sev_scaled)\n",
    "\n",
    "# Print Accuracy\n",
    "print(\"Severity Model - MAE:\", mean_absolute_error(y_test_sev, y_pred_sev))\n",
    "print(\"Severity Model - R²:\", r2_score(y_test_sev, y_pred_sev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623784d-9d51-4737-ba2e-bd35c6f0d4ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "I got your model to run too...finally haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebf613de-c215-4a4e-b9a4-76af1a73168a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=1000, tol=0.3; total time=   0.9s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=3000, tol=0.1; total time=   0.6s\n",
      "[CV] END alpha=0.0001, hidden_layer_sizes=(300,), max_iter=2000, tol=0.2; total time=   0.7s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=2000, tol=0.3; total time=   0.8s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=3000, tol=0.1; total time=   0.8s\n",
      "[CV] END alpha=0.0001, hidden_layer_sizes=(300,), max_iter=2000, tol=0.2; total time=   0.8s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=2000, tol=0.3; total time=   0.9s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(100,), max_iter=3000, tol=0.3; total time=   0.3s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=3000, tol=0.1; total time=   0.6s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=2000, tol=0.1; total time=   0.8s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=1000, tol=0.3; total time=   0.8s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=1000, tol=0.3; total time=   0.5s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=2000, tol=0.1; total time=   0.6s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(200,), max_iter=2000, tol=0.2; total time=   0.5s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=2000, tol=0.3; total time=   0.8s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=1000, tol=0.3; total time=   0.5s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=2000, tol=0.1; total time=   0.6s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(200,), max_iter=2000, tol=0.2; total time=   0.6s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=3000, tol=0.1; total time=   0.9s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(100,), max_iter=3000, tol=0.3; total time=   0.3s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=3000, tol=0.1; total time=   0.5s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=2000, tol=0.1; total time=   0.8s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=1000, tol=0.3; total time=   0.9s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(100,), max_iter=3000, tol=0.3; total time=   0.4s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=2000, tol=0.1; total time=   0.5s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=2000, tol=0.1; total time=   0.8s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(300,), max_iter=3000, tol=0.1; total time=   0.8s\n",
      "[CV] END alpha=0.001, hidden_layer_sizes=(200,), max_iter=1000, tol=0.3; total time=   0.6s\n",
      "[CV] END alpha=0.0001, hidden_layer_sizes=(300,), max_iter=2000, tol=0.2; total time=   0.8s\n",
      "[CV] END alpha=0.005, hidden_layer_sizes=(200,), max_iter=2000, tol=0.2; total time=   0.5s\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mean_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m                 scores \u001b[38;5;241m=\u001b[39m cross_val_score(regr, X_train, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m#gets cross validation score\u001b[39;00m\n\u001b[1;32m     16\u001b[0m                 cv_scores\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m:mi,\n\u001b[1;32m     17\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_layer_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m:hls,\n\u001b[1;32m     18\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m\"\u001b[39m:t,\n\u001b[1;32m     19\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m:a})\n\u001b[0;32m---> 21\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#find the parameters with the highest (best) cv score\u001b[39;00m\n\u001b[1;32m     22\u001b[0m best_params\n",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 scores \u001b[38;5;241m=\u001b[39m cross_val_score(regr, X_train, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m#gets cross validation score\u001b[39;00m\n\u001b[1;32m     16\u001b[0m                 cv_scores\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m:mi,\n\u001b[1;32m     17\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_layer_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m:hls,\n\u001b[1;32m     18\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m\"\u001b[39m:t,\n\u001b[1;32m     19\u001b[0m                                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m:a})\n\u001b[0;32m---> 21\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cv_scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;66;03m#find the parameters with the highest (best) cv score\u001b[39;00m\n\u001b[1;32m     22\u001b[0m best_params\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mean_accuracy'"
     ]
    }
   ],
   "source": [
    "# this takes forever maybe don't run it\n",
    "regr = MLPRegressor(random_state=1, solver='lbfgs')\n",
    "parameters = {\"max_iter\": list(range(2000,5000,1000)), \n",
    "              \"hidden_layer_sizes\": list(range(100, 500, 100)),\n",
    "             \"tol\": [0.1,0.2,0.3,0.4,0.5],\n",
    "             \"alpha\": [0.0001, 0.0005, 0.001, 0.005, 0.01],}\n",
    "cv_scores = [] #holds the cross validation scores\n",
    "#try every combination of estimators and max depth, recording them and their scores\n",
    "for mi in parameters['max_iter']:\n",
    "    for hls in parameters['hidden_layer_sizes']:\n",
    "        for t in parameters['tol']:\n",
    "            for a in parameters['alpha']:\n",
    "                regr.set_params(solver='lbfgs', max_iter=mi, hidden_layer_sizes=hls, tol=t, alpha=a)\n",
    "                scores = cross_val_score(regr, X_train, y_train, cv=5) #gets cross validation score\n",
    "                \n",
    "                cv_scores.append({\"max_iter\":mi,\n",
    "                                 \"hidden_layer_sizes\":hls,\n",
    "                                 \"tol\":t,\n",
    "                                 \"alpha\":a})\n",
    "\n",
    "best_params = max(cv_scores, key=lambda x: x[\"mean_accuracy\"]) #find the parameters with the highest (best) cv score\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "facf2107-f4fa-4c61-af90-813694b3f6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tol': 0.2, 'max_iter': 2000, 'hidden_layer_sizes': (300,), 'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8bea24c-a24f-45e3-8162-796580f67686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model - MAE: 0.3328002139531869\n",
      "Final Model - R²: 0.04840067663652403\n"
     ]
    }
   ],
   "source": [
    "# Extract best parameters\n",
    "best_max_iter = best_params[\"max_iter\"]\n",
    "best_hidden_layers = best_params[\"hidden_layer_sizes\"]\n",
    "best_tol = best_params[\"tol\"]\n",
    "best_alpha = best_params[\"alpha\"]\n",
    "\n",
    "# Train final model using best parameters\n",
    "best_model = MLPRegressor(solver='lbfgs', \n",
    "                          max_iter=best_max_iter, \n",
    "                          hidden_layer_sizes=best_hidden_layers, \n",
    "                          tol=best_tol, \n",
    "                          alpha=best_alpha,\n",
    "                          random_state=1)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Final Model - MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Final Model - R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06385a0d-41f3-42a4-92f3-297c608bb852",
   "metadata": {},
   "source": [
    "I think we should ask how to make this faster because...it took me like 30 minutes to run all of this or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce58ba-6fc0-474c-8649-97f76b9385c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B-25W] *",
   "language": "python",
   "name": "conda-env-PIC16B-25W-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
