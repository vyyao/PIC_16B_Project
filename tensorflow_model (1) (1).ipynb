{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd1e38a-90bd-45ea-9451-be526e8ce768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import express as px\n",
    "import plotly.graph_objs as go\n",
    "# ML libraries - idk which one to use yet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, mean_absolute_error, r2_score\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import cKDTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedc2883-6e80-41c9-aec4-5f6d35eb4648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NASA API\n",
    "def get_nasa_power_data(lat, lon, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches NASA POWER API data for given latitude, longitude, and time range.\n",
    "\n",
    "    Args:\n",
    "    - lat (float): Latitude of the location.\n",
    "    - lon (float): Longitude of the location.\n",
    "    - start_date (str): Start date in YYYYMMDD format.\n",
    "    - end_date (str): End date in YYYYMMDD format.\n",
    "\n",
    "    Returns:\n",
    "    - Pandas DataFrame with selected weather parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Specify multiple parameters in the API request\n",
    "    parameters = \"PRECSNO,T2MDEW,PRECTOTCORR,T2M,WS2M\"\n",
    "\n",
    "    url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "    params = {\n",
    "        \"parameters\": parameters,\n",
    "        \"community\": \"RE\",\n",
    "        \"longitude\": lon,\n",
    "        \"latitude\": lat,\n",
    "        \"start\": start_date,\n",
    "        \"end\": end_date,\n",
    "        \"format\": \"JSON\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # Convert JSON response to DataFrame and transpose it\n",
    "    nasa_weather = pd.DataFrame.from_dict(data[\"properties\"][\"parameter\"], orient=\"index\").T\n",
    "\n",
    "    # Reset index and rename date column\n",
    "    nasa_weather.reset_index(inplace=True)\n",
    "    nasa_weather.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "\n",
    "    # Convert date column to proper datetime format\n",
    "    nasa_weather[\"date\"] = pd.to_datetime(nasa_weather[\"date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    nasa_weather.dropna(subset=[\"date\"], inplace=True)  # Remove invalid date rows\n",
    "\n",
    "    nasa_weather.rename(columns={\n",
    "        \"PRECSNO\": \"Snow_Precipitation\",\n",
    "        \"T2MDEW\": \"Dew_Point_2m\",\n",
    "        \"PRECTOTCORR\": \"Total_Precipitation_mm\",\n",
    "        \"T2M\": \"Temperature_2m_C\",\n",
    "        \"WS2M\": \"Wind_Speed_2m\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Add Rounded_Lat and Rounded_Lng for merging\n",
    "    nasa_weather['Rounded_Lat'] = lat\n",
    "    nasa_weather['Rounded_Lng'] = lon\n",
    "    \n",
    "    # Display DataFrame\n",
    "    print(f\"\\n Weather Data for Latitude {lat}, Longitude {lon}\\n\")\n",
    "    print(f\"\\n Weather Data for Latitude {lat}, Longitude {lon}\\n\")\n",
    "    nasa_weather['Precipitation(in)'] = nasa_weather['Total_Precipitation_mm'] / 25.4 # mm to in\n",
    "    nasa_weather['Temperature(F)'] = (nasa_weather['Temperature_2m_C'] * (9./5.)) + 32. # C to F\n",
    "    nasa_weather['Wind_Speed(mph)'] = nasa_weather['Wind_Speed_2m'] * 2.237 # m/s to mph\n",
    "    # nasa_weather.dropna()\n",
    "    display(nasa_weather)  # Works in Jupyter Notebook\n",
    "\n",
    "    return nasa_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9938d54-cead-4cf2-a65f-b238d91be651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weather Data for Latitude 34.05, Longitude -118.25\n",
      "\n",
      "\n",
      " Weather Data for Latitude 34.05, Longitude -118.25\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Snow_Precipitation</th>\n",
       "      <th>Dew_Point_2m</th>\n",
       "      <th>Total_Precipitation_mm</th>\n",
       "      <th>Temperature_2m_C</th>\n",
       "      <th>Wind_Speed_2m</th>\n",
       "      <th>Rounded_Lat</th>\n",
       "      <th>Rounded_Lng</th>\n",
       "      <th>Precipitation(in)</th>\n",
       "      <th>Temperature(F)</th>\n",
       "      <th>Wind_Speed(mph)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>12.14</td>\n",
       "      <td>1.64</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>53.852</td>\n",
       "      <td>3.66868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>11.55</td>\n",
       "      <td>1.59</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>52.790</td>\n",
       "      <td>3.55683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>5.87</td>\n",
       "      <td>10.86</td>\n",
       "      <td>3.25</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.231102</td>\n",
       "      <td>51.548</td>\n",
       "      <td>7.27025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9.76</td>\n",
       "      <td>2.89</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>49.568</td>\n",
       "      <td>6.46493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.440</td>\n",
       "      <td>4.47400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10.50</td>\n",
       "      <td>2.86</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>50.900</td>\n",
       "      <td>6.39782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>7.77</td>\n",
       "      <td>5.29</td>\n",
       "      <td>34.05</td>\n",
       "      <td>-118.25</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>45.986</td>\n",
       "      <td>11.83373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Snow_Precipitation  Dew_Point_2m  Total_Precipitation_mm  \\\n",
       "0 2024-01-01                 0.0          6.12                    0.04   \n",
       "1 2024-01-02                 0.0          7.05                    0.09   \n",
       "2 2024-01-03                 0.0          6.75                    5.87   \n",
       "3 2024-01-04                 0.0          2.14                    0.02   \n",
       "4 2024-01-05                 0.0          1.99                    0.00   \n",
       "5 2024-01-06                 0.0          1.23                    0.12   \n",
       "6 2024-01-07                 0.0         -0.09                    0.26   \n",
       "\n",
       "   Temperature_2m_C  Wind_Speed_2m  Rounded_Lat  Rounded_Lng  \\\n",
       "0             12.14           1.64        34.05      -118.25   \n",
       "1             11.55           1.59        34.05      -118.25   \n",
       "2             10.86           3.25        34.05      -118.25   \n",
       "3              9.76           2.89        34.05      -118.25   \n",
       "4             10.80           2.00        34.05      -118.25   \n",
       "5             10.50           2.86        34.05      -118.25   \n",
       "6              7.77           5.29        34.05      -118.25   \n",
       "\n",
       "   Precipitation(in)  Temperature(F)  Wind_Speed(mph)  \n",
       "0           0.001575          53.852          3.66868  \n",
       "1           0.003543          52.790          3.55683  \n",
       "2           0.231102          51.548          7.27025  \n",
       "3           0.000787          49.568          6.46493  \n",
       "4           0.000000          51.440          4.47400  \n",
       "5           0.004724          50.900          6.39782  \n",
       "6           0.010236          45.986         11.83373  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Fetch data for different locations\n",
    "df_la = get_nasa_power_data(34.05, -118.25, \"20240101\", \"20240107\")  # Los Angeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa465c-3a70-4b5f-bf78-ccc26b0b5fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect us_accident data\n",
    "us_accidents = pd.read_csv('US_Accidents_March23.csv')\n",
    "us_accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4b6a1-e2da-40a3-939c-10d46e3eca18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# string feature that if kept will need to be encoded for ML\n",
    "str_features = 'Weather_Condition'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb65272-e239-40b6-bddc-58541cd87e73",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943e9b9-a13c-4062-bf3b-1a621babe445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define broad weather categories\n",
    "weather_mapping = {\n",
    "    \"Rain\": [\"Rain\", \"Drizzle\", \"Showers\", \"Thunderstorms\", \"Light Rain\", \"Heavy Rain\", \"Scattered Showers\"],\n",
    "    \"Snow\": [\"Snow\", \"Blowing Snow\", \"Sleet\", \"Heavy Snow\", \"Flurries\", \"Light Snow\", \"Freezing Rain\", \"Snow Showers\"],\n",
    "    \"Windy\": [\"Windy\", \"Gusty Winds\", \"Blowing Dust\", \"Dust Storm\", \"Strong Wind\"],\n",
    "    \"Thunderstorm\": [\"Thunderstorm\", \"T-Storm\", \"Heavy Thunderstorms\", \"Severe Thunderstorms\"],\n",
    "    \"Fog/Haze\": [\"Fog\", \"Haze\", \"Mist\", \"Patchy Fog\", \"Dense Fog\"],\n",
    "    \"Clear/Sunny\": [\"Clear\", \"Fair\", \"Sunny\", \"Partly Cloudy\", \"Mostly Clear\"],\n",
    "    \"Cloudy\": [\"Cloudy\", \"Overcast\", \"Mostly Cloudy\", \"Partly Cloudy\"],\n",
    "}\n",
    "\n",
    "# Function to categorize weather conditions\n",
    "def categorize_weather(condition):\n",
    "    if pd.isna(condition):  # Check for NaN values\n",
    "        return \"Other\"\n",
    "    \n",
    "    condition = str(condition)  # Ensure it's a string\n",
    "    for category, keywords in weather_mapping.items():\n",
    "        if any(keyword in condition for keyword in keywords):\n",
    "            return category\n",
    "    return \"Other\"  # Default category for uncategorized conditions\n",
    "\n",
    "# Convert NaNs and apply categorization\n",
    "us_accidents[\"Weather_Condition\"] = us_accidents[\"Weather_Condition\"].astype(str).fillna(\"Unknown\")\n",
    "us_accidents[\"Weather_Category\"] = us_accidents[\"Weather_Condition\"].apply(categorize_weather)\n",
    "\n",
    "# Use the correct DataFrame name (should match `us_accidents`)\n",
    "weather_severity_grouped = us_accidents.groupby(\"Weather_Category\")[\"Severity\"].mean().reset_index()\n",
    "\n",
    "# Sort by severity\n",
    "weather_severity_grouped = weather_severity_grouped.sort_values(by=\"Severity\", ascending=False)\n",
    "\n",
    "# Plot with improved readability\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    x=\"Weather_Category\",\n",
    "    y=\"Severity\",\n",
    "    data=weather_severity_grouped,\n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "\n",
    "# Improve readability\n",
    "plt.xticks(rotation=45, fontsize=12, ha=\"right\")  # Rotate labels for clarity\n",
    "plt.title(\"Average Accident Severity by Weather Category\", fontsize=14)\n",
    "plt.xlabel(\"Weather Category\", fontsize=12)\n",
    "plt.ylabel(\"Average Severity\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096bc68-afe5-40a2-ab0c-258bdb40d616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=\"Weather_Category\", y=\"Severity\", data=us_accidents, palette=\"coolwarm\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Accident Severity Distribution by Weather Category\")\n",
    "plt.xlabel(\"Weather Category\")\n",
    "plt.ylabel(\"Severity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010956b-576d-4877-9cf6-82c8225e006c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_severity_distribution(df):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.countplot(x=us_accidents['Severity'], palette='viridis')\n",
    "    plt.title('Distribution of Accident Severity', fontsize=14)\n",
    "    plt.xlabel('Severity', fontsize=12)\n",
    "    plt.ylabel('Number of Accidents', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "plot_severity_distribution(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff337c-c56d-49d0-840a-d8d371cffb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_binary_features_vs_severity(df, features):\n",
    "    fig, axes = plt.subplots(1, len(features), figsize=(15,5))\n",
    "    for i, feature in enumerate(features):\n",
    "        sns.barplot(x=df[feature], y=us_accidents['Severity'], ax=axes[i], palette='pastel')\n",
    "        axes[i].set_title(f'{feature} vs Severity')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "road_features = ['Junction', 'Traffic_Signal', 'Crossing', 'Bump']\n",
    "plot_binary_features_vs_severity(df, road_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024b922-afc8-403a-8055-e1a67e545433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "us_accidents['Start_Time'] = us_accidents['Start_Time'].str.replace(r'\\.\\d+$', '', regex=True) # removes the fractional seconds (.000000000)\n",
    "us_accidents['End_Time'] = us_accidents['End_Time'].str.replace(r'\\.\\d+$', '', regex=True) \n",
    "\n",
    "us_accidents['Start_Time'] = pd.to_datetime(us_accidents['Start_Time']) \n",
    "us_accidents['End_Time'] = pd.to_datetime(us_accidents['End_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716180d-c202-431f-9bdf-4d662f31beb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "incident_counts_by_time = us_accidents['Start_Time'].dt.hour.value_counts().sort_index()\n",
    "incident_counts_by_day = us_accidents['Start_Time'].dt.day_name().value_counts().sort_index()\n",
    "incident_counts_by_week = us_accidents['Start_Time'].dt.isocalendar().week.value_counts().sort_index()\n",
    "incident_counts_by_month = us_accidents['Start_Time'].dt.month_name().value_counts().sort_index()\n",
    "incident_counts_by_year = ['Start_Time'].dt.year.value_counts().sort_index()\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "axs[0, 0].bar(incident_counts_by_time.index, incident_counts_by_time.values, color='skyblue')\n",
    "axs[0, 0].set_title('Number of Incidents by Hour of the Day', fontsize=14)\n",
    "axs[0, 0].set_xlabel('Hour of the Day', fontsize=12)\n",
    "axs[0, 0].set_ylabel('Number of Incidents', fontsize=12)\n",
    "\n",
    "axs[0, 1].bar(incident_counts_by_day.index, incident_counts_by_day.values, color='lightgreen')\n",
    "axs[0, 1].set_title('Number of Incidents by Day of the Week', fontsize=14)\n",
    "axs[0, 1].set_xlabel('Day of the Week', fontsize=12)\n",
    "axs[0, 1].set_ylabel('Number of Incidents', fontsize=12)\n",
    "\n",
    "axs[1, 0].bar(incident_counts_by_week.index.astype(int), incident_counts_by_week.values, color='salmon')\n",
    "axs[1, 0].set_title('Number of Incidents by Week of the Year', fontsize=14)\n",
    "axs[1, 0].set_xlabel('Week of the Year', fontsize=12)\n",
    "axs[1, 0].set_ylabel('Number of Incidents', fontsize=12)\n",
    "\n",
    "axs[1, 1].bar(incident_counts_by_month.index, incident_counts_by_month.values, color='lightcoral')\n",
    "axs[1, 1].set_title('Number of Incidents by Month', fontsize=14)\n",
    "axs[1, 1].set_xlabel('Month', fontsize=12) \n",
    "axs[1, 1].set_ylabel('Number of Incidents', fontsize=12)\n",
    "axs[1, 1].tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "axs[2, 0].bar(incident_counts_by_year.index, incident_counts_by_year.values, color='lightblue')\n",
    "axs[2, 0].set_title('Number of Incidents by Year', fontsize=14)\n",
    "axs[2, 0].set_xlabel('Year', fontsize=12)\n",
    "axs[2, 0].set_ylabel('Number of Incidents', fontsize=12)\n",
    "\n",
    "axs[2, 1].axis('off')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16eccb-af42-4f91-b3df-dcdbf8ac563f",
   "metadata": {},
   "source": [
    "# Deep Learning using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57d6c3-abd1-4744-a871-1e401bf3aee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of all columns in US Accident Data\n",
    "drop_cols = ['ID',\n",
    "            'Source',\n",
    "            # 'Severity', # Severity = target column, 1-4, where 1 indicates the least impact on traffic\n",
    "            'Start_Time',\n",
    "            'End_Time',\n",
    "            'Start_Lat',  \n",
    "            'Start_Lng', \n",
    "            'End_Lat',\n",
    "            'End_Lng',\n",
    "            'Distance(mi)', # Distance(mi) = target column?, length of road extent affected by accident in miles\n",
    "            'Description', # Description = human description of accident\n",
    "            'Street', \n",
    "            'City', \n",
    "            'County',\n",
    "            'State',\n",
    "            'Zipcode',\n",
    "            'Country',\n",
    "            'Timezone',\n",
    "            'Airport_Code',\n",
    "            'Weather_Timestamp', # Weather_Timestamp = shows time-stamp of weather observation record (in local time)\n",
    "            # 'Temperature(F)',\n",
    "            'Wind_Chill(F)',\n",
    "            'Humidity(%)',\n",
    "            'Pressure(in)',\n",
    "            'Visibility(mi)',\n",
    "            'Wind_Direction',\n",
    "            # 'Wind_Speed(mph)',\n",
    "            # 'Precipitation(in)',\n",
    "            'Weather_Condition',\n",
    "            'Amenity',\n",
    "            'Bump',\n",
    "            'Crossing',\n",
    "            'Give_Way',\n",
    "            'Junction',\n",
    "            'No_Exit',\n",
    "            'Railway',\n",
    "            'Roundabout',\n",
    "            'Station',\n",
    "            'Stop',\n",
    "            'Traffic_Calming',\n",
    "            'Traffic_Signal',\n",
    "            'Turning_Loop',\n",
    "            'Sunrise_Sunset', # day or night based on sunrise/sunset\n",
    "            'Civil_Twilight', # day or night based on civil twilight\n",
    "            'Nautical_Twilight', # day or night based on nautical twilight\n",
    "            'Astronomical_Twilight'] # day or night based on astronomical twilight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392196e6-5ad4-4765-987d-ff54fe162e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def prepare_data(df, split=False, predictors=[], target=[]):\n",
    "    \"\"\"\n",
    "    Prepares the US Accidents DataFrame for analysis, keeping only necessary columns.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Raw US Accidents dataset.\n",
    "    - split (boolean): If True, splits df into target and predictor data.\n",
    "    - predictors (list): Columns used as predictors.\n",
    "    - target (list): Target column for ML models.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Convert time columns to datetime format if they exist\n",
    "    if 'Start_Time' in df.columns:\n",
    "        df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "\n",
    "    if 'End_Time' in df.columns:\n",
    "        df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n",
    "\n",
    "    # Remove rows with invalid 'Start_Time' values\n",
    "    if 'Start_Time' in df.columns:\n",
    "        df = df[df['Start_Time'].notnull()].copy()\n",
    "\n",
    "    # Extract relevant time features\n",
    "    if 'Start_Time' in df.columns:\n",
    "        df['Hour_of_day'] = df['Start_Time'].dt.hour\n",
    "        df['Day_of_week'] = df['Start_Time'].dt.dayofweek\n",
    "        df['Month'] = df['Start_Time'].dt.month\n",
    "        df['Is_Weekend'] = df['Day_of_week'].isin([5, 6]).astype(int)  # 1 if Sat/Sun, 0 otherwise\n",
    "\n",
    "    # Drop unnecessary weather-related and redundant columns\n",
    "    drop_cols = [\n",
    "        'ID', 'Source', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', \n",
    "        'Distance(mi)', 'Description', 'Street', 'City', 'County', 'State', 'Zipcode', 'Country', \n",
    "        'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Weather_Condition', 'Wind_Direction',\n",
    "        'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)'\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Convert categorical binary features into numerical\n",
    "    binary_features = [\n",
    "        'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout',\n",
    "        'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop'\n",
    "    ]\n",
    "    for col in binary_features:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "    # Create a \"Complex Road\" feature indicating the presence of multiple road features\n",
    "    complex_road_features = ['Junction', 'Railway', 'Crossing']\n",
    "    if all(col in df.columns for col in complex_road_features):\n",
    "        df['Is_Complex_Road'] = df[complex_road_features].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)\n",
    "    else:\n",
    "        df['Is_Complex_Road'] = 0\n",
    "\n",
    "    # Convert twilight and lighting conditions into binary\n",
    "    twilight_cols = ['Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "    for col in twilight_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: 1 if x in ['Yes', 'Day'] else 0)\n",
    "\n",
    "    # Calculate accident duration in seconds and apply log transformation if 'End_Time' exists\n",
    "    if 'End_Time' in df.columns and 'Start_Time' in df.columns:\n",
    "        df['Duration'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds()\n",
    "        df['Duration'] = np.log1p(df['Duration'])\n",
    "    else:\n",
    "        df['Duration'] = np.nan  # Avoid errors if 'End_Time' is missing\n",
    "\n",
    "    # Drop NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Return processed dataset\n",
    "    if split:\n",
    "        X = df[predictors]\n",
    "        y = df[target]\n",
    "        return X, y\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ac616-80dc-47e6-9ce0-a2a034f4a20d",
   "metadata": {},
   "source": [
    "Trying new neural network based off of https://www.kaggle.com/code/kelixirr/us-accidents-severity-prediction-end-to-end#Preparing-Our-Data-For-The-Model\n",
    "\n",
    "(i had to download tensorflow)\n",
    "\n",
    "prepare_data was changed to only have locaiton and 3 weather attributes (the same that can be obtained from nasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb724f-e070-4860-bdaf-6335a67f8e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340995de-b9a6-414b-8319-a6bde99439eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you get ValueError: The least populated class in y has only 1 member, \n",
    "# which is too few. The minimum number of groups for any class cannot be less than 2.\n",
    "# just re-run until it works (idk why)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(us_accidents, test_size=0.2)\n",
    "df_train = prepare_data(train, split=False)\n",
    "# predictors: ['Rounded_Lat','Rounded_Lng','Temperature(F)','Wind_Speed(mph)','Precipitation(in)']\n",
    "X = df_train.drop(columns=['Severity']) \n",
    "# predicting severity of an accident at this location with current weather conditions\n",
    "y = df_train['Severity'] \n",
    "\n",
    "# make training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=30, stratify=y)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=30, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab8be1-b726-49ec-9328-b0ce4b852276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect predictors\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3562d-f107-48d4-a947-1b77b71c0b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check y_values (severity labels) are in range [0,4] not negative or other\n",
    "y_values = y_test.unique()\n",
    "print(y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cc853-58ba-41d5-ae18-26183bb8d99f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train = y_train - 1\n",
    "y_valid = y_valid - 1  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid_scaled, y_valid))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "valid_dataset = valid_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a103f7-083c-4938-94ac-6199cd3efbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_values = train['Severity'].unique()\n",
    "print(unique_values) # 4 unique class for accident severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e07f95-c70e-4a76-b39f-fe7a11969abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape = (X_train_scaled.shape[1],)))\n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(4, activation='softmax'))  # 4 unique classes for accident severity\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20bf72-7930-4655-8f42-837d29a883da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * (drop ** np.floor((1+epoch)/epochs_drop))\n",
    "    return lr\n",
    "\n",
    "# create checkpoint to save best model during training\n",
    "checkpoint_path = 'best_model.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=50,\n",
    "                    validation_data=valid_dataset,\n",
    "                    callbacks=[checkpoint, early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f493a75-abe7-40ff-82ec-a1847ec20e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.path.isfile('best_model.keras') # make sure true before loading saved models in later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846333cf-e4b0-48a4-91ab-57611902f486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model = tf.keras.models.load_model(\"best_model.keras\")\n",
    "val_loss, val_accuracy = saved_model.evaluate(valid_dataset) \n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3908791-e760-44e6-a469-b394bf096eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test = y_test - 1\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test)) \n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe44a4b-1b5d-476a-bc51-8baba132315d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = saved_model.evaluate(test_dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60dd1f-464d-4af4-88e3-759cdaf55b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = np.argmax(saved_model.predict(X_test_scaled), axis=1) \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cf24b-9fa7-4caa-9e3e-a0186d025370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = get_nasa_power_data(34.05, -118.25, \"20240101\", \"20240107\")\n",
    "df = df_test[['Temperature(F)','Wind_Speed(mph)','Precipitation(in)','Rounded_Lat','Rounded_Lng']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46afe86-b4b0-4fbd-9f1d-578e028ed73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_test = scaler.transform(df)\n",
    "test = np.argmax(saved_model.predict(nasa_test), axis=1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4962d-8990-480a-a9ac-e88f3b8ed2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Ensure y_test and y_pred are correctly formatted\n",
    "y_pred = np.argmax(saved_model.predict(X_test_scaled), axis=1)\n",
    "\n",
    "# Compute Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Compute R-squared (R²) score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R²) Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b14e4-bb8a-4508-b172-2251e38ddd8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_severity(latitude, longitude, current_time):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        - latitude, double coordinate from map\n",
    "        - longitude, double coordinate from map\n",
    "        - current_time, str representing current date Pacific Standard Time\n",
    "    Return: \n",
    "        - none, prints severity estimate\n",
    "    \"\"\"\n",
    "    nasa_data = get_nasa_power_date(latitude, longitude, current_time)\n",
    "    nasa_scaled = scaler.transform(df)\n",
    "    saved_model = tf.keras.models.load_model(\"best_model.keras\")\n",
    "    severity = np.argmax(saved_model.predict(nasa_scaled), axis=1)\n",
    "    print(f\"The current weather is: \")\n",
    "    print(f\"    - Temperature (F): {nasa_data['Temperature(F)']}\")\n",
    "    print(f\"    - Wind Speed (mph): {nasa_data['Wind_Speed(mph)']}\")\n",
    "    print(f\"    - Precipitation (in): {nasa_data['Precipitation(in)']}\")\n",
    "    print(f\"The estimated accident severity for this location and weather conditions is: {severity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721583eb-9de1-47d8-9a0e-dc0623da1816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add in necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plotly import express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from dash import Dash, dcc, html, dash_table, Input, Output, State, callback, no_update\n",
    "\n",
    "import base64\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "import io\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from plotly import express as px\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "import dash_bootstrap_components as dbc\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813000f-42bb-4316-9379-8d158015411b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dash import Dash, html, Input, Output, Patch\n",
    "import dash_leaflet as dl\n",
    "import json\n",
    "\n",
    "\n",
    "app = Dash()\n",
    "app.layout = html.Div([\n",
    "    \n",
    "    dcc.Markdown('''\n",
    "        ## Directions: \n",
    "        Pan and zoom in/out on the map to find your location.\n",
    "        Click your location on the map for current weather risk estimation.\n",
    "        Your risk estimates will load promptly.'''),\n",
    "\n",
    "    \n",
    "    dcc.Markdown('''## Select Location:'''),\n",
    "\n",
    "    dl.Map(\n",
    "        id='map',\n",
    "        children=[\n",
    "            dl.TileLayer()\n",
    "        ],\n",
    "        center=[34, -118],\n",
    "        zoom=9,\n",
    "        style={'height': '50vh'}\n",
    "    ),\n",
    "    html.Div(id='out')\n",
    "    ]\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('out', 'children'),\n",
    "    Output('map', 'children'),\n",
    "    Input('map', 'clickData'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def map(click_data):\n",
    "    # extract coordinates form click_data\n",
    "    coordinates = click_data['latlng']\n",
    "    latitude, longitude = coordinates.values()\n",
    "    current_date = date.today().strftime('%Y%m%d')\n",
    "    estimate_severity(latitude, longitude, current_date)\n",
    "\n",
    "    # create Patch() instance, add Marker layer at click coordinates\n",
    "    patched = Patch()\n",
    "    patched.append(dl.Marker(position=[latitude, longitude]))\n",
    "\n",
    "    return json.dumps(coordinates), patched\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=1050,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ff984-a8d3-4ccc-91a6-643caf7e6a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bae75-13a0-4005-8126-fe59e6ab40a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
